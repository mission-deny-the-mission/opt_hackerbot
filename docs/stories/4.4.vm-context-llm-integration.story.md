# Story 4.4: Integrate VM Context into LLM Prompt Assembly

**Status**: Draft  
**Epic**: Epic 4: VM Context Fetching from Student Machines  
**Priority**: High  
**Estimated Effort**: 3-4 days  
**Dependencies**: Story 4.3

---

## Story

**As a** developer,  
**I want** VM context integrated into the LLM prompt assembly system alongside RAG context, attack context, and chat history,  
**so that** the LLM can provide responses based on actual student machine state information.

---

## Acceptance Criteria

1. VM context included in `get_enhanced_context` return value
2. `assemble_prompt` incorporates VM context into final LLM prompt
3. VM context clearly distinguished from other context types (RAG, attack, chat)
4. Context ordering: VM context appears in logical position (e.g., after attack context, before RAG)
5. Context length management respects max_context_length limits (truncate VM context if needed)
6. Option to enable/disable VM context injection per bot or attack (configuration flag)
7. Integration tests verify VM context appears in LLM prompts
8. End-to-end tests verify LLM responses incorporate VM state information

---

## Tasks / Subtasks

- [ ] Modify `get_enhanced_context` method to include VM context (AC: 1)
  - [ ] Add VM context fetching call in `get_enhanced_context` method (around line 483)
  - [ ] Fetch VM context using `fetch_vm_context(bot_name, attack_index)` from Story 4.3
  - [ ] Add VM context to enhanced_context hash return value
  - [ ] Include VM context only if attack has VM context config
- [ ] Update `assemble_prompt` method to incorporate VM context (AC: 2)
  - [ ] Add `vm_context` parameter to `assemble_prompt` method signature (or extract from enhanced_context)
  - [ ] Include VM context section in prompt assembly
  - [ ] Maintain existing prompt structure for other context types
  - [ ] Format VM context with clear section header
- [ ] Ensure VM context is clearly distinguished (AC: 3)
  - [ ] Use clear section header: "VM State:" or "Student Machine State:"
  - [ ] Format VM context with distinct formatting from RAG/chat context
  - [ ] Add source attribution within VM context sections
  - [ ] Ensure LLM can distinguish VM state from other context
- [ ] Implement logical context ordering (AC: 4)
  - [ ] Order: System Prompt → Attack Context → VM Context → Enhanced Context (RAG) → Chat History → User Message
  - [ ] Ensure VM context appears after attack context but before RAG context
  - [ ] Maintain readability of prompt structure
- [ ] Implement context length management for VM context (AC: 5)
  - [ ] Check `max_context_length` limits
  - [ ] Truncate VM context if total prompt exceeds limits
  - [ ] Prioritize recent bash history and recent command outputs
  - [ ] Truncate file contents if needed (keep first N lines)
  - [ ] Log warnings when truncation occurs
- [ ] Add configuration flag for VM context enable/disable (AC: 6)
  - [ ] Add `vm_context_enabled` flag to bot-level config
  - [ ] Add optional `vm_context_enabled` flag to attack-level config
  - [ ] Check flags before fetching VM context
  - [ ] Default: VM context enabled if configured (opt-in via XML config)
- [ ] Create integration test for prompt assembly (AC: 7)
  - [ ] Test VM context included in enhanced_context hash
  - [ ] Test VM context appears in assembled prompt string
  - [ ] Test VM context section formatting
  - [ ] Test context ordering is correct
  - [ ] Test with and without VM context config
- [ ] Create end-to-end test (AC: 8)
  - [ ] Test complete flow: message → VM context fetch → prompt assembly → LLM response
  - [ ] Verify LLM response references VM state information
  - [ ] Test with different VM context configurations
  - [ ] Test error scenarios (VM context fetch fails)

---

## Dev Notes

### Previous Story Insights
- Story 4.3: `fetch_vm_context` and `assemble_vm_context` methods created in `bot_manager.rb`
- VM context is fetched from student VMs via SSH
- VM context formatted as structured string with bash history, commands, and files

### Technical Details

#### Prompt Assembly Flow (current)
1. System prompt (personality or attack-specific)
2. Attack context (current attack prompt)
3. Enhanced context (RAG results) - if enabled
4. Chat history (IRC conversation)
5. User message
6. Assistant prompt

#### Prompt Assembly Flow (with VM context)
1. System prompt
2. Attack context
3. **VM Context** (NEW - student machine state)
4. Enhanced context (RAG results)
5. Chat history
6. User message
7. Assistant prompt

#### Integration Points

**Location 1: `get_enhanced_context` method** (`bot_manager.rb` line 483)
- Current return structure:
```ruby
{
  original_query: "...",
  rag_context: {...},
  explicit_context: {...},
  combined_context: "...",
  sources: [...]
}
```
- Extended structure (add VM context):
```ruby
{
  original_query: "...",
  rag_context: {...},
  explicit_context: {...},
  vm_context: "VM State:\n...",  # NEW
  combined_context: "...",
  sources: [...]
}
```

**Location 2: `assemble_prompt` method** (`bot_manager.rb` line 1752)
- Current signature: `assemble_prompt(system_prompt, context, chat_context, user_message, enhanced_context = nil)`
- Modify to extract VM context from `enhanced_context` hash or add separate parameter
- Add VM context section to prompt string

#### File Locations

**Files to Modify**:
- `bot_manager.rb` - Modify `get_enhanced_context` method (around line 483)
- `bot_manager.rb` - Modify `assemble_prompt` method (around line 1752)
- `bot_manager.rb` - Add VM context fetching call in message handling (around line 2119)

**Files to Create**:
- `test/test_vm_context_prompt_integration.rb` - Integration tests
- `test/test_vm_context_e2e.rb` - End-to-end tests

#### Data Models and Structures

**Enhanced Context Hash Structure** (extended):
```ruby
enhanced_context = {
  original_query: "user message",
  rag_context: {...},
  explicit_context: {...},
  vm_context: "VM State:\nBash History:\n...",  # Formatted string from Story 4.3
  combined_context: "...",
  sources: [...],
  timestamp: Time.now
}
```

**Prompt Structure** (with VM context):
```
{system_prompt}

Context: {attack_context}

VM State:
{vm_context_string}

Enhanced Context:
{rag_context}

Chat History:
{chat_context}

User: {user_message}
Assistant:
```

#### API Specifications

**Modified `get_enhanced_context` Method**:
```ruby
def get_enhanced_context(bot_name, user_message, attack_index: nil)
  # ... existing RAG/explicit context logic ...
  
  # Fetch VM context if attack has config
  vm_context = nil
  if attack_index && @bots[bot_name]['attacks'][attack_index]&.dig('vm_context')
    begin
      vm_context = fetch_vm_context(bot_name, attack_index)
    rescue => e
      Print.warn "Failed to fetch VM context: #{e.message}"
    end
  end
  
  # Add VM context to enhanced_context hash
  enhanced_context[:vm_context] = vm_context if vm_context
  
  enhanced_context
end
```

**Modified `assemble_prompt` Method**:
```ruby
def assemble_prompt(system_prompt, context, chat_context, user_message, enhanced_context = nil)
  vm_context = enhanced_context&.dig(:vm_context)
  
  # Build prompt with VM context section
  sections = [system_prompt]
  sections << "Context: #{context}" unless context.empty?
  sections << "VM State:\n#{vm_context}" if vm_context && !vm_context.strip.empty?
  sections << "Enhanced Context:\n#{enhanced_context[:combined_context]}" if enhanced_context&.dig(:combined_context)
  sections << "Chat History:\n#{chat_context}" unless chat_context.empty?
  sections << "User: #{user_message}"
  sections << "Assistant:"
  
  sections.join("\n\n")
end
```

#### Component Specifications

**Context Length Management**:
- Check total prompt length against `get_max_context_length`
- Truncate VM context if prompt exceeds limit
- Prioritize: Recent bash history > Recent commands > File contents
- Truncate strategy: Keep first portion, add "... (truncated)" message

**Configuration Flags**:
```ruby
# Bot-level (optional in XML)
@bots[bot_name]['vm_context_enabled'] = true/false

# Attack-level (optional in XML)
attack_data['vm_context_enabled'] = true/false

# Default: Enable if vm_context config exists, else disabled
```

#### File Locations Based on Project Structure

[Source: docs/architecture/source-tree.md]

- **Main implementation**: `bot_manager.rb` - Modify existing methods
- **Tests**: `test/test_vm_context_prompt_integration.rb` - Integration tests
- **E2E Tests**: `test/test_vm_context_e2e.rb` - End-to-end tests

#### Testing Requirements

[Source: docs/architecture/coding-standards.md#testing-standards]

**Integration Test Structure**:
```ruby
require 'test_helper'

class VMContextPromptIntegrationTest < Minitest::Test
  def test_vm_context_in_enhanced_context
    # Verify VM context included in get_enhanced_context return
  end
  
  def test_vm_context_in_assembled_prompt
    # Verify VM context appears in assembled prompt string
  end
  
  def test_context_ordering
    # Verify VM context appears in correct position
  end
  
  def test_vm_context_formatting
    # Verify VM context section is clearly formatted
  end
end
```

**E2E Test Structure**:
```ruby
require 'test_helper'

class VMContextE2ETest < Minitest::Test
  def test_vm_context_in_llm_response
    # Mock LLM client, verify response references VM state
  end
  
  def test_complete_flow_with_vm_context
    # Test: message → fetch → prompt → LLM → response
  end
end
```

#### Technical Constraints

- **Backward Compatibility**: Prompts without VM context must work unchanged
- **Performance**: VM context fetching should not significantly delay responses
- **Length Limits**: Respect existing max_context_length constraints
- **Error Handling**: Bot must continue if VM context integration fails

#### Security Considerations

- **Context Leakage**: Ensure VM context doesn't expose sensitive data (sanitization in Story 4.5)
- **Prompt Injection**: VM context content should be properly formatted to avoid prompt injection

#### Integration Points

- **Uses**: `fetch_vm_context` from Story 4.3
- **Integrates With**: `get_enhanced_context` and `assemble_prompt` existing methods
- **Modifies**: Message handling flow (around line 2119-2122)
- **Reads**: VM context config from Story 4.2

---

## Testing

### Testing Standards
[Source: docs/architecture/coding-standards.md#testing-standards]

- **Test File Location**: `test/test_vm_context_prompt_integration.rb` (integration), `test/test_vm_context_e2e.rb` (e2e)
- **Test Framework**: Minitest (Ruby standard library)
- **Test Naming**: `test_*` methods describing what they test
- **Mock Usage**: Mock VMContextManager and LLM clients for testing
- **Test Coverage**: All integration paths and edge cases

### Specific Test Cases

1. **Test VM Context in Enhanced Context**
   - Verify VM context added to enhanced_context hash
   - Test with and without VM context config
   - Test when VM context fetch fails

2. **Test VM Context in Prompt Assembly**
   - Verify VM context appears in assembled prompt
   - Test prompt structure and formatting
   - Test context ordering (VM context position)

3. **Test Context Length Management**
   - Test VM context truncation when prompt too long
   - Test priority ordering (bash history > commands > files)
   - Test truncation logging

4. **Test Configuration Flags**
   - Test bot-level VM context enable/disable
   - Test attack-level VM context enable/disable
   - Test default behavior (enabled when config exists)

5. **Test End-to-End Flow**
   - Test complete message → VM context → prompt → LLM flow
   - Verify LLM response references VM state
   - Test error scenarios (VM fetch fails)

6. **Test Backward Compatibility**
   - Attacks without VM context work unchanged
   - Prompts without VM context maintain existing structure
   - No errors when VM context disabled

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-XX | 1.0 | Initial story creation | Scrum Master |

---

## Dev Agent Record

_(To be populated by development agent during implementation)_

### Agent Model Used
_(To be populated)_

### Debug Log References
_(To be populated)_

### Completion Notes List
_(To be populated)_

### File List
_(To be populated)_

---

## QA Results

_(To be populated by QA agent after story implementation review)_

