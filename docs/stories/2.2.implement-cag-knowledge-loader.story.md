# Story 2.2: Implement Knowledge Base Loader for CAG

**Epic**: Epic 2 - CAG System Implementation
**Story ID**: 2.2
**Priority**: Critical
**Estimated Effort**: 3-4 days
**Dependencies**: Story 2.1 (CAG architecture design complete)

---

## Status

**Draft**

---

## Story

**As a** developer,
**I want** to implement a knowledge base loader for the CAG system,
**so that** I can load, preprocess, and optimize knowledge base documents for CAG context assembly with efficient chunking and prioritization strategies.

---

## Acceptance Criteria

1. Implementation file created: cag/knowledge_loader.rb
2. Loader loads documents from all knowledge source types (MITRE ATT&CK, man pages, markdown)
3. Document preprocessing implemented with text cleaning and normalization
4. Chunking strategy implemented with configurable chunk size and overlap
5. Document prioritization logic implemented for context window optimization
6. Metadata extraction and preservation for all document types
7. Configuration system supports customization of loading parameters
8. Integration with existing knowledge_bases/sources/ components verified
9. Error handling for malformed documents and loading failures implemented
10. All functionality tested with unit tests

---

## Integration Verification

- **IV1**: Verify loader uses same knowledge sources as RAG system (shared components)
- **IV2**: Verify loader doesn't modify original knowledge base files
- **IV3**: Verify preprocessing output suitable for long-context LLM consumption
- **IV4**: Verify memory usage reasonable during document loading (<2GB for typical knowledge base)

---

## Tasks / Subtasks

### Phase 1: Core Loader Infrastructure (AC: 1, 8)

- [ ] **Task 1.1: Create Knowledge Loader Class Structure**
  - [ ] Create cag/knowledge_loader.rb file
  - [ ] Examine existing knowledge_bases/sources/ architecture
  - [ ] Review Story 2.1 architecture design specifications
  - [ ] Define KnowledgeLoader class with initialization and configuration
  - [ ] Implement configuration hash handling (chunk_size, overlap, prioritization settings)
  - [ ] Set up logging with Print utility class

- [ ] **Task 1.2: Integrate with Existing Knowledge Sources**
  - [ ] Examine knowledge_bases/sources/ directory structure
  - [ ] Identify common interfaces for knowledge sources
  - [ ] Implement loader integration for MITRE ATT&CK source
  - [ ] Implement loader integration for man page source
  - [ ] Implement loader integration for markdown source
  - [ ] Create abstraction for adding new knowledge sources

- [ ] **Task 1.3: Implement Base Document Loading**
  - [ ] Create load_documents method for all knowledge sources
  - [ ] Implement document iteration and collection
  - [ ] Extract document content and metadata
  - [ ] Create internal document representation structure
  - [ ] Handle loading errors with graceful degradation
  - [ ] Log loading progress and statistics

### Phase 2: Document Preprocessing (AC: 3, 6)

- [ ] **Task 2.1: Implement Text Cleaning**
  - [ ] Create clean_document method for text normalization
  - [ ] Remove excessive whitespace and newlines
  - [ ] Normalize special characters and encoding
  - [ ] Handle code blocks and formatting preservation
  - [ ] Preserve important structural elements (headers, lists)
  - [ ] Test with various document types

- [ ] **Task 2.2: Implement Metadata Extraction**
  - [ ] Extract metadata for MITRE ATT&CK documents (ID, tactics, techniques)
  - [ ] Extract metadata for man pages (command name, section, synopsis)
  - [ ] Extract metadata for markdown (headers, structure, topic)
  - [ ] Create standardized metadata format for all document types
  - [ ] Implement metadata validation and sanitization
  - [ ] Store metadata with processed documents

- [ ] **Task 2.3: Implement Document Structuring**
  - [ ] Identify document sections (intro, content, examples, etc.)
  - [ ] Preserve document hierarchy and relationships
  - [ ] Create section-level metadata for prioritization
  - [ ] Handle nested structures (subsections, lists)
  - [ ] Maintain source references for traceability
  - [ ] Test structure preservation with complex documents

### Phase 3: Chunking Strategy Implementation (AC: 4, 5)

- [ ] **Task 3.1: Implement Basic Chunking**
  - [ ] Create chunk_document method with configurable chunk size
  - [ ] Implement fixed-size chunking algorithm
  - [ ] Implement overlap mechanism between chunks
  - [ ] Preserve chunk boundaries at sentence/paragraph breaks
  - [ ] Calculate optimal chunk size for target context windows
  - [ ] Test chunking with various document lengths

- [ ] **Task 3.2: Implement Semantic Chunking**
  - [ ] Identify semantic boundaries (sections, topics)
  - [ ] Implement semantic-aware chunking algorithm
  - [ ] Preserve complete concepts within chunks
  - [ ] Handle code blocks and examples as atomic units
  - [ ] Implement chunk size balancing (avoid very small/large chunks)
  - [ ] Compare semantic vs fixed chunking performance

- [ ] **Task 3.3: Implement Chunk Metadata and Tracking**
  - [ ] Assign unique IDs to each chunk
  - [ ] Track chunk position in original document
  - [ ] Store chunk overlap information
  - [ ] Calculate chunk relevance scores
  - [ ] Create chunk-level metadata (type, size, source)
  - [ ] Enable chunk reconstruction for verification

### Phase 4: Document Prioritization (AC: 5, 7)

- [ ] **Task 4.1: Implement Relevance Scoring**
  - [ ] Create prioritize_documents method
  - [ ] Implement cybersecurity domain relevance scoring
  - [ ] Calculate document importance based on metadata
  - [ ] Score chunks based on content type (examples > theory)
  - [ ] Weight technical content higher than general descriptions
  - [ ] Test scoring with representative queries

- [ ] **Task 4.2: Implement Context Window Optimization**
  - [ ] Calculate context window budget for target models
  - [ ] Implement document selection based on priority and size
  - [ ] Optimize chunk ordering for LLM consumption
  - [ ] Handle context window overflow gracefully
  - [ ] Implement document truncation strategies
  - [ ] Verify optimal context utilization (>80% of available window)

- [ ] **Task 4.3: Implement Configurable Prioritization Strategies**
  - [ ] Support multiple prioritization modes (relevance, recency, importance)
  - [ ] Enable domain-specific prioritization rules
  - [ ] Implement custom scoring functions via configuration
  - [ ] Allow prioritization override for specific use cases
  - [ ] Document prioritization algorithm and parameters
  - [ ] Test with different configuration scenarios

### Phase 5: Configuration and Error Handling (AC: 7, 9)

- [ ] **Task 5.1: Implement Configuration System**
  - [ ] Define configuration schema for knowledge loader
  - [ ] Support chunk_size, chunk_overlap, max_chunks parameters
  - [ ] Support prioritization_mode, scoring_weights parameters
  - [ ] Support knowledge_source_paths configuration
  - [ ] Implement configuration validation with sensible defaults
  - [ ] Create example configuration for common use cases

- [ ] **Task 5.2: Implement Error Handling and Logging**
  - [ ] Handle malformed documents with error logging
  - [ ] Gracefully skip corrupted or unreadable files
  - [ ] Implement retry logic for transient failures
  - [ ] Log detailed error information with Print.err
  - [ ] Provide error summaries and statistics
  - [ ] Ensure partial loading failures don't crash system

- [ ] **Task 5.3: Implement Performance Monitoring**
  - [ ] Measure and log document loading time
  - [ ] Track memory usage during preprocessing
  - [ ] Monitor chunking performance
  - [ ] Log statistics (documents loaded, chunks created, errors)
  - [ ] Implement performance thresholds and warnings
  - [ ] Create loading performance report

### Phase 6: Testing and Validation (AC: 10, IV1-IV4)

- [ ] **Task 6.1: Create Unit Tests for Knowledge Loader**
  - [ ] Create test/test_cag_knowledge_loader.rb
  - [ ] Set up test fixtures for each knowledge source type
  - [ ] Test document loading from all source types
  - [ ] Test preprocessing and cleaning functionality
  - [ ] Test chunking strategies (fixed, semantic, overlap)
  - [ ] Test metadata extraction and preservation

- [ ] **Task 6.2: Test Prioritization and Optimization**
  - [ ] Test relevance scoring algorithm
  - [ ] Test context window optimization
  - [ ] Test different prioritization configurations
  - [ ] Verify chunk ordering is logical and optimal
  - [ ] Test with varying context window sizes
  - [ ] Validate memory usage stays within limits

- [ ] **Task 6.3: Integration Verification Tests** (IV1-IV4)
  - [ ] **IV1**: Test loader uses same knowledge sources as RAG
  - [ ] **IV1**: Verify no modifications to original source files
  - [ ] **IV2**: Verify read-only access to knowledge bases
  - [ ] **IV3**: Verify preprocessed output format suitable for LLMs
  - [ ] **IV3**: Test output with sample LLM prompts
  - [ ] **IV4**: Measure and verify memory usage (<2GB threshold)

- [ ] **Task 6.4: Error Handling and Edge Case Tests**
  - [ ] Test with empty knowledge base
  - [ ] Test with very large documents (>100KB)
  - [ ] Test with malformed/corrupted documents
  - [ ] Test with missing metadata
  - [ ] Test with invalid configuration parameters
  - [ ] Verify graceful degradation in all error scenarios

---

## Dev Notes

### CAG Knowledge Loader Architecture

**Purpose**: Load and preprocess knowledge base documents for CAG system context assembly

**Key Responsibilities**:
- Load documents from multiple knowledge source types
- Clean and normalize document text for LLM consumption
- Chunk documents into optimal sizes for long-context windows
- Extract and preserve metadata for prioritization
- Prioritize documents/chunks based on relevance and importance
- Optimize document selection for context window constraints

[Source: docs/stories/epic-2-cag-system-implementation.md#42-65, Story 2.1 design]

### Knowledge Source Integration

**Existing Knowledge Sources** (knowledge_bases/sources/):
- MITRE ATT&CK framework (techniques, tactics, procedures)
- Man pages (system documentation)
- Markdown files (training materials, lab sheets)

**Integration Approach**:
- Reuse existing knowledge source loading interfaces
- Shared knowledge base between RAG and CAG systems
- No modifications to original source files (read-only access)

[Source: docs/development/architecture.md, knowledge_bases/ directory]

### Document Chunking Strategies

**Fixed-Size Chunking**:
```ruby
def chunk_document_fixed(document, chunk_size: 2000, overlap: 200)
  chunks = []
  text = document[:content]
  position = 0

  while position < text.length
    chunk_end = [position + chunk_size, text.length].min
    chunk_text = text[position...chunk_end]

    chunks << {
      id: "chunk_#{chunks.length}",
      text: chunk_text,
      position: position,
      metadata: document[:metadata].merge(chunk_index: chunks.length)
    }

    position += chunk_size - overlap
  end

  chunks
end
```

**Semantic Chunking** (preserve meaning):
```ruby
def chunk_document_semantic(document, max_chunk_size: 2000)
  # Split on section boundaries, paragraphs, sentences
  # Preserve complete concepts within chunks
  # Handle code blocks as atomic units
end
```

[Source: CAG research papers, Story 2.1 design specifications]

### Document Prioritization Logic

**Relevance Scoring Factors**:
1. **Document Type**: Technical content > theory
2. **Metadata Importance**: Command examples, attack techniques
3. **Recency**: Newer content weighted higher
4. **Cybersecurity Domain**: Security-specific content prioritized
5. **Completeness**: Complete examples vs fragments

**Priority Calculation**:
```ruby
def calculate_priority(document)
  score = 0.0

  # Type weighting
  score += 2.0 if document[:type] == :mitre_attack
  score += 1.5 if document[:type] == :man_page
  score += 1.0 if document[:type] == :markdown

  # Content analysis
  score += 2.0 if contains_code_examples?(document)
  score += 1.5 if contains_commands?(document)
  score += 1.0 if contains_techniques?(document)

  # Metadata scoring
  score += metadata_score(document[:metadata])

  score
end
```

[Source: docs/prd.md#Epic 2 requirements, cybersecurity training focus]

### Context Window Optimization

**Target Context Sizes**:
- Small models: 32k tokens (~24k usable)
- Medium models: 64k tokens (~48k usable)
- Large models: 128k tokens (~96k usable)

**Optimization Strategy**:
1. Prioritize documents by relevance score
2. Select highest-priority documents within budget
3. Truncate or summarize if necessary
4. Reserve space for conversation history
5. Target 80-90% context window utilization

**Token Estimation**:
```ruby
def estimate_tokens(text)
  # Rough estimation: ~4 characters per token
  (text.length / 4.0).ceil
end

def optimize_for_context_window(documents, max_tokens: 24000)
  sorted_docs = documents.sort_by { |doc| -doc[:priority] }
  selected = []
  total_tokens = 0

  sorted_docs.each do |doc|
    doc_tokens = estimate_tokens(doc[:content])
    break if total_tokens + doc_tokens > max_tokens

    selected << doc
    total_tokens += doc_tokens
  end

  selected
end
```

[Source: docs/stories/epic-2-cag-system-implementation.md#382-407]

### Configuration Schema

```ruby
# Example configuration for KnowledgeLoader
config = {
  knowledge_sources: [
    'knowledge_bases/sources/mitre_attack',
    'knowledge_bases/sources/man_pages',
    'knowledge_bases/sources/markdown'
  ],

  chunking: {
    strategy: :semantic,      # :fixed, :semantic, :hybrid
    chunk_size: 2000,         # characters
    chunk_overlap: 200,       # characters
    preserve_code_blocks: true
  },

  prioritization: {
    mode: :relevance,         # :relevance, :recency, :importance
    scoring_weights: {
      document_type: 1.0,
      technical_content: 2.0,
      metadata_importance: 1.5
    },
    cybersecurity_focus: true
  },

  context_window: {
    max_tokens: 24000,        # Target model context size
    utilization_target: 0.85, # Use 85% of available window
    reserve_for_history: 4000 # Tokens for conversation
  },

  performance: {
    max_memory_mb: 2048,      # Memory limit for loading
    enable_caching: true,
    log_statistics: true
  }
}

loader = CAG::KnowledgeLoader.new(config)
```

[Source: Story 2.1 architecture design, docs/prd.md#Epic 2 NFRs]

### Metadata Schema

**Standardized Metadata Format**:
```ruby
{
  # Common fields
  source_type: :mitre_attack,  # :man_page, :markdown
  source_path: 'path/to/original/file',
  document_id: 'T1003',
  title: 'Credential Dumping',

  # Type-specific fields
  mitre_attack: {
    technique_id: 'T1003',
    tactics: ['credential-access'],
    platforms: ['Windows', 'Linux']
  },

  man_page: {
    command: 'ls',
    section: 1,
    synopsis: 'ls [OPTION]... [FILE]...'
  },

  markdown: {
    headers: ['Lab 1: Port Scanning', 'Objective'],
    tags: ['nmap', 'reconnaissance']
  },

  # Derived fields
  priority_score: 7.5,
  chunk_count: 3,
  total_tokens: 1523,
  created_at: Time.now
}
```

[Source: RAG system metadata patterns, CAG requirements]

### Performance Targets

**Loading Performance** (NFR9):
- Document loading: <30 seconds for typical knowledge base (1000+ documents)
- Preprocessing: <60 seconds total
- Chunking: <15 seconds
- Prioritization: <5 seconds
- Memory usage: <2GB during loading

**Quality Metrics**:
- Chunk coherence: Semantic boundaries preserved
- Context utilization: >80% of available window
- Metadata accuracy: 100% preservation
- Error handling: Graceful degradation, no crashes

[Source: docs/prd.md#2.2 NFR7-NFR12]

### Error Handling Patterns

```ruby
def load_documents_safe(source_path)
  documents = []

  begin
    source_files = Dir.glob(File.join(source_path, '**/*'))

    source_files.each do |file|
      begin
        doc = load_document(file)
        documents << doc if doc
      rescue DocumentLoadError => e
        Print.err "Failed to load document #{file}: #{e.message}"
        # Continue with other documents
      end
    end

  rescue StandardError => e
    Print.err "Knowledge source loading error: #{e.message}"
    Print.err "Backtrace: #{e.backtrace.join("\n")}"
    # Return partial results
  end

  Print.info "Loaded #{documents.length} documents from #{source_path}"
  documents
end
```

[Source: Existing error handling patterns in RAG system]

### Testing Strategy

**Unit Test Coverage**:
- Document loading: All source types
- Preprocessing: Text cleaning, normalization
- Chunking: Fixed, semantic, overlap strategies
- Metadata: Extraction, validation, preservation
- Prioritization: Scoring algorithms, selection
- Configuration: Validation, defaults
- Error handling: All error paths

**Test Fixtures**:
- Sample MITRE ATT&CK techniques
- Sample man pages (ls, grep, nmap)
- Sample markdown training materials
- Malformed/corrupted documents
- Very large documents (>100KB)
- Empty documents

**Performance Tests**:
- Load 1000+ documents
- Measure memory usage
- Measure processing time
- Verify optimization targets met

[Source: Story 1.1 test patterns, Epic 2 requirements]

### Integration with CAG System

**Knowledge Loader Output Format**:
```ruby
{
  documents: [
    {
      id: 'doc_001',
      content: 'processed text...',
      metadata: { ... },
      priority: 7.5,
      tokens: 1523,
      chunks: [
        {
          id: 'chunk_001_0',
          text: 'chunk text...',
          position: 0,
          overlap_with_next: 200
        }
      ]
    }
  ],

  statistics: {
    total_documents: 150,
    total_chunks: 450,
    total_tokens: 125000,
    loading_time_ms: 2500,
    memory_used_mb: 512
  }
}
```

**Integration Points**:
- Input: knowledge_bases/sources/ (shared with RAG)
- Output: Preprocessed documents for context_manager.rb (Story 2.3)
- Configuration: CAG system configuration hash
- Logging: Print utility for structured logging

[Source: Story 2.1 architecture design, Epic 2 integration requirements]

---

## Testing

### Testing Strategy for This Story

**Unit Testing Approach**:
1. **Component Tests**: Test each method independently
2. **Integration Tests**: Test with real knowledge sources
3. **Performance Tests**: Verify memory and time constraints
4. **Error Tests**: Validate error handling and edge cases

### Test File Structure

```ruby
# test/test_cag_knowledge_loader.rb
require_relative 'test_helper'
require_relative '../cag/knowledge_loader'

class TestCAGKnowledgeLoader < Minitest::Test
  def setup
    @config = {
      knowledge_sources: ['test/fixtures/knowledge_sources'],
      chunking: { strategy: :semantic, chunk_size: 1000 },
      prioritization: { mode: :relevance }
    }
    @loader = CAG::KnowledgeLoader.new(@config)
  end

  def teardown
    # Cleanup if needed
  end

  def test_load_mitre_attack_documents
    # Test MITRE ATT&CK loading
  end

  def test_chunking_preserves_semantic_boundaries
    # Test semantic chunking
  end

  def test_prioritization_scoring
    # Test priority calculation
  end

  # ... more tests
end
```

### Test Execution Commands

```bash
# Run knowledge loader tests
ruby test/test_cag_knowledge_loader.rb

# Run with verbose output
ruby test/test_cag_knowledge_loader.rb --verbose

# Run in Nix environment
nix develop
ruby test/test_cag_knowledge_loader.rb

# Run all CAG tests (as they become available)
ruby test/run_tests.rb --pattern "test_cag_*.rb"
```

### Success Criteria

✅ All unit tests pass
✅ Integration with existing knowledge sources verified
✅ Chunking strategies work correctly
✅ Prioritization produces logical ordering
✅ Memory usage stays under 2GB
✅ Error handling graceful for all scenarios
✅ Configuration system flexible and validated

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | v1.0 | Initial story creation | SM Agent (Bob) |

---

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

---

## QA Results

_This section will be populated by QA Agent after story completion._

---

**Story prepared by**: Scrum Master Agent (Bob)
**Ready for**: Developer Agent implementation (after Story 2.1 completion)
**Next Story**: 2.3 - Implement Context Manager (depends on this story completion)
