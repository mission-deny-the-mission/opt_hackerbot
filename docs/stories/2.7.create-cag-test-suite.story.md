# Story 2.7: Create Comprehensive CAG Test Suite

**Epic**: Epic 2 - CAG System Implementation
**Story ID**: 2.7
**Priority**: High
**Estimated Effort**: 3-4 days
**Dependencies**: Story 2.6

---

## Status

**Draft**

---

## Story

**As a** developer,
**I want** to create thorough automated tests for the CAG system,
**so that** I can validate its correctness, cache management, and context optimization functionality.

---

## Acceptance Criteria

1. Test file created: test/test_cag_comprehensive.rb
2. Tests cover knowledge base loading and preprocessing for CAG context assembly
3. Tests verify context manager functionality (document assembly, optimization, compression)
4. Tests verify cache manager functionality (KV cache precomputation, storage, retrieval, invalidation)
5. Tests validate inference engine response generation with cached context
6. Tests verify CAG manager coordinator integration of all components
7. Edge cases tested: empty knowledge bases, cache misses, large contexts, cache invalidation scenarios
8. Test coverage ≥80% for cag/cag_manager.rb and related CAG components
9. All tests pass in Nix development environment

---

## Integration Verification

- **IV1**: Verify tests don't modify production knowledge bases or caches
- **IV2**: Verify test execution time reasonable (≤10 minutes for full CAG test suite)
- **IV3**: Verify tests can run offline (no external API dependencies for core functionality)
- **IV4**: Verify tests don't interfere with existing RAG system functionality

---

## Tasks / Subtasks

### Phase 1: Test Infrastructure Setup (AC: 1)

- [ ] **Task 1.1: Create Test File and Framework Setup**
  - [ ] Create test/test_cag_comprehensive.rb
  - [ ] Examine existing test patterns in test/test_helper.rb and test/test_rag_comprehensive.rb
  - [ ] Set up test framework (Minitest based on existing tests)
  - [ ] Create test class: `TestCAGComprehensive < Minitest::Test`
  - [ ] Add setup and teardown methods

- [ ] **Task 1.2: Create Test Fixtures**
  - [ ] Create test/fixtures/cag_test_data/ directory
  - [ ] Create sample knowledge base documents (cybersecurity-focused)
  - [ ] Create sample multi-turn conversation contexts
  - [ ] Create test cache configurations (various sizes and document counts)
  - [ ] Create expected context assembly samples (for verification)

- [ ] **Task 1.3: Set Up Test Configuration**
  - [ ] Create test CAG configuration (isolated from production)
  - [ ] Configure test cache storage (temporary directory)
  - [ ] Configure test LLM client (Ollama with long-context model, or mock)
  - [ ] Ensure tests use separate cache files from production
  - [ ] Set up test knowledge base paths

### Phase 2: Knowledge Base Loading Tests (AC: 2)

- [ ] **Task 2.1: Test Knowledge Base Preprocessing**
  - [ ] Test loading knowledge sources for CAG
  - [ ] Verify document preprocessing and chunking
  - [ ] Verify document prioritization strategies
  - [ ] Verify metadata preservation during preprocessing
  - [ ] Test loading with empty/malformed knowledge bases

- [ ] **Task 2.2: Test Document Chunking Strategies**
  - [ ] Test semantic chunking for optimal context usage
  - [ ] Verify chunk size optimization
  - [ ] Test overlap preservation between chunks
  - [ ] Verify chunk boundaries respect document structure
  - [ ] Test with various document types (markdown, man pages, MITRE data)

- [ ] **Task 2.3: Test Document Prioritization**
  - [ ] Test relevance-based document ranking
  - [ ] Verify cybersecurity domain weighting
  - [ ] Test prioritization with different query contexts
  - [ ] Verify critical documents always included
  - [ ] Test with knowledge bases exceeding context window

### Phase 3: Context Manager Tests (AC: 3)

- [ ] **Task 3.1: Test Context Assembly**
  - [ ] Test document assembly into context window
  - [ ] Verify context formatting for LLM consumption
  - [ ] Test context window size management
  - [ ] Verify document ordering in assembled context
  - [ ] Test context assembly with different knowledge base sizes

- [ ] **Task 3.2: Test Context Optimization**
  - [ ] Test context compression techniques
  - [ ] Verify optimal context window utilization
  - [ ] Test relevance-based document selection
  - [ ] Verify context quality after compression
  - [ ] Test with various context window sizes (32k, 64k, 128k tokens)

- [ ] **Task 3.3: Test Multi-Turn Conversation Context**
  - [ ] Test conversation history integration
  - [ ] Verify context updates for follow-up queries
  - [ ] Test context trimming for long conversations
  - [ ] Verify important context preservation across turns
  - [ ] Test context switching for different topics

### Phase 4: Cache Manager Tests (AC: 4)

- [ ] **Task 4.1: Test KV Cache Precomputation**
  - [ ] Test cache precomputation for knowledge bases
  - [ ] Verify cache generation completes within time limits (≤5 minutes)
  - [ ] Test cache computation with different model configurations
  - [ ] Verify cache integrity after precomputation
  - [ ] Test error handling during cache computation

- [ ] **Task 4.2: Test Cache Storage and Retrieval**
  - [ ] Test cache persistence to disk
  - [ ] Verify cache loading from storage
  - [ ] Test cache loading time (≤30 seconds)
  - [ ] Verify memory-efficient cache representation
  - [ ] Test cache compression (if implemented)

- [ ] **Task 4.3: Test Cache Invalidation**
  - [ ] Test cache invalidation on knowledge base updates
  - [ ] Verify partial cache invalidation for incremental updates
  - [ ] Test cache versioning and compatibility
  - [ ] Verify automatic cache refresh triggers
  - [ ] Test manual cache clearing operations

- [ ] **Task 4.4: Test Cache Memory Management**
  - [ ] Measure cache memory usage
  - [ ] Verify memory usage within limits (≤6GB for typical knowledge bases)
  - [ ] Test cache eviction strategies (if implemented)
  - [ ] Test memory-mapped cache loading (if implemented)
  - [ ] Verify no memory leaks during cache operations

### Phase 5: Inference Engine Tests (AC: 5)

- [ ] **Task 5.1: Test Query Processing with Cached Context**
  - [ ] Test query processing using preloaded cache
  - [ ] Verify context injection into inference
  - [ ] Test response generation with cached knowledge
  - [ ] Verify query latency meets targets (≤2 seconds)
  - [ ] Test with various query types (simple, complex, multi-part)

- [ ] **Task 5.2: Test Response Quality**
  - [ ] Test response accuracy with cached context
  - [ ] Verify responses use cached knowledge appropriately
  - [ ] Test response consistency across multiple runs
  - [ ] Verify source attribution in responses
  - [ ] Test fallback behavior when cache unavailable

- [ ] **Task 5.3: Test Inference Performance**
  - [ ] Measure inference latency with CAG
  - [ ] Compare to RAG inference latency (target: 50-80% improvement)
  - [ ] Test concurrent inference requests
  - [ ] Verify performance consistency over time
  - [ ] Test cache hit rates during inference

### Phase 6: CAG Manager Integration Tests (AC: 6)

- [ ] **Task 6.1: Test CAG Manager Initialization**
  - [ ] Test CAG manager setup and configuration
  - [ ] Verify all components initialized correctly
  - [ ] Test with various configuration options
  - [ ] Verify error handling during initialization
  - [ ] Test graceful degradation when components unavailable

- [ ] **Task 6.2: Test End-to-End CAG Workflow**
  - [ ] Test complete workflow: load → cache → query → respond
  - [ ] Verify seamless integration of all CAG components
  - [ ] Test workflow with different knowledge base configurations
  - [ ] Verify workflow performance meets all targets
  - [ ] Test error propagation and handling across components

- [ ] **Task 6.3: Test CAG-RAG Coexistence**
  - [ ] Verify CAG doesn't interfere with RAG system
  - [ ] Test switching between CAG and RAG modes
  - [ ] Verify independent operation of both systems
  - [ ] Test shared knowledge base loading
  - [ ] Verify no resource conflicts between systems

### Phase 7: Edge Case and Error Handling Tests (AC: 7)

- [ ] **Task 7.1: Test Empty/Invalid Inputs**
  - [ ] Test with empty knowledge base
  - [ ] Test with empty query string
  - [ ] Test with null inputs
  - [ ] Test with very long queries (>1000 characters)
  - [ ] Verify graceful error handling

- [ ] **Task 7.2: Test Cache Miss Scenarios**
  - [ ] Test behavior when cache not precomputed
  - [ ] Verify fallback to on-demand caching
  - [ ] Test cache corruption handling
  - [ ] Verify error messages for cache issues
  - [ ] Test recovery from cache failures

- [ ] **Task 7.3: Test Large Context Scenarios**
  - [ ] Test with knowledge bases approaching context limit
  - [ ] Test with knowledge bases exceeding context window
  - [ ] Verify context truncation strategies
  - [ ] Test memory usage with maximum context sizes
  - [ ] Verify performance doesn't degrade with large contexts

- [ ] **Task 7.4: Test Cache Invalidation Edge Cases**
  - [ ] Test concurrent cache access during invalidation
  - [ ] Test cache updates while queries in progress
  - [ ] Test rapid sequential cache invalidations
  - [ ] Verify cache consistency after failed invalidation
  - [ ] Test cache rollback scenarios

- [ ] **Task 7.5: Test Error Scenarios**
  - [ ] Test with LLM service unavailable
  - [ ] Test with insufficient memory for cache
  - [ ] Test with corrupted knowledge base files
  - [ ] Verify all errors logged with Print.err
  - [ ] Verify system doesn't crash on errors

### Phase 8: Coverage and Integration (AC: 8, 9)

- [ ] **Task 8.1: Measure Test Coverage**
  - [ ] Run tests with coverage tool (SimpleCov or similar)
  - [ ] Verify ≥80% coverage for cag/cag_manager.rb
  - [ ] Verify coverage for cag/context_manager.rb
  - [ ] Verify coverage for cag/cache_manager.rb
  - [ ] Verify coverage for cag/knowledge_loader.rb
  - [ ] Verify coverage for cag/inference_engine.rb
  - [ ] Identify untested code paths and add tests

- [ ] **Task 8.2: Run All Tests in Nix Environment**
  - [ ] Execute full test suite: `ruby test/test_cag_comprehensive.rb`
  - [ ] Verify all tests pass
  - [ ] Fix any failures
  - [ ] Verify tests complete in ≤10 minutes
  - [ ] Document any environment-specific requirements

- [ ] **Task 8.3: Integration Verification** (IV1, IV2, IV3, IV4)
  - [ ] **IV1**: Verify tests use isolated test data, not production
  - [ ] **IV2**: Measure and document total test execution time
  - [ ] **IV3**: Run tests offline (disable network), verify pass
  - [ ] **IV4**: Run RAG tests alongside CAG tests, verify no conflicts
  - [ ] Verify tests clean up after themselves (no temp files/caches left)

---

## Dev Notes

### CAG System Architecture

**CAG Manager** (`cag/cag_manager.rb`):
- Main coordinator for CAG operations
- Methods to test: initialization, knowledge loading coordination, cache management, query processing
- [Source: docs/stories/2.6.implement-cag-manager.story.md]

**Context Manager** (`cag/context_manager.rb`):
- Document assembly and context optimization
- Methods to test: context assembly, optimization, compression, multi-turn handling
- [Source: docs/stories/2.3.implement-context-manager.story.md]

**Cache Manager** (`cag/cache_manager.rb`):
- KV cache precomputation and management
- Methods to test: cache precomputation, storage, retrieval, invalidation, memory management
- [Source: docs/stories/2.4.implement-cache-manager.story.md]

**Knowledge Loader** (`cag/knowledge_loader.rb`):
- Knowledge base preprocessing for CAG
- Methods to test: document loading, preprocessing, chunking, prioritization
- [Source: docs/stories/2.2.implement-cag-knowledge-loader.story.md]

**Inference Engine** (`cag/inference_engine.rb`):
- Response generation with cached context
- Methods to test: query processing, context injection, response generation, performance
- [Source: docs/stories/2.5.implement-cag-inference-engine.story.md]

### Existing Test Patterns

**Test Framework**: Ruby Minitest (based on existing tests)
[Source: test/test_helper.rb, test/test_rag_comprehensive.rb]

**Test File Pattern**:
```ruby
require_relative 'test_helper'

class TestCAGComprehensive < Minitest::Test
  def setup
    # Initialize test CAG manager
  end

  def teardown
    # Clean up test resources (caches, temp files)
  end

  def test_something
    # Test implementation
  end
end
```

**Test Execution**:
```bash
ruby test/test_cag_comprehensive.rb
# or
ruby test/run_tests.rb  # Run all tests
```
[Source: Existing test files]

### Test Configuration Example

```ruby
def setup
  @test_cache_dir = File.join(Dir.tmpdir, "cag_test_cache_#{Process.pid}")
  Dir.mkdir(@test_cache_dir) unless Dir.exist?(@test_cache_dir)

  @cag_config = {
    context_manager: {
      max_context_tokens: 32768,
      chunk_size: 512,
      chunk_overlap: 50
    },
    cache_manager: {
      cache_dir: @test_cache_dir,
      cache_version: 'test_v1',
      precompute_timeout: 300  # 5 minutes
    },
    knowledge_loader: {
      source_paths: ['test/fixtures/cag_test_data'],
      prioritization_strategy: 'relevance'
    },
    inference_engine: {
      llm_provider: 'ollama',
      model: 'qwen2.5:32k',  # Long-context model for testing
      max_tokens: 1024
    },
    collection_name: 'test_cag_collection'  # Separate from production
  }

  @cag_manager = CAGManager.new(@cag_config)
end

def teardown
  @cag_manager.cleanup if @cag_manager
  FileUtils.rm_rf(@test_cache_dir) if @test_cache_dir && Dir.exist?(@test_cache_dir)
end
```

### Knowledge Source Test Data

**Cybersecurity Test Documents**:
```ruby
# Sample documents for CAG testing
test_knowledge_base = [
  {
    title: 'Port Scanning Techniques',
    content: 'Nmap is a powerful network scanner...',
    category: 'reconnaissance',
    priority: 'high'
  },
  {
    title: 'Credential Dumping with Mimikatz',
    content: 'Mimikatz is a post-exploitation tool...',
    category: 'credential-access',
    priority: 'high'
  },
  {
    title: 'SQL Injection Basics',
    content: 'SQL injection exploits vulnerabilities in database queries...',
    category: 'exploitation',
    priority: 'medium'
  }
  # ... more test documents
]
```

**Multi-Turn Conversation Test Data**:
```ruby
test_conversation = [
  { role: 'user', content: 'How do I perform a port scan?' },
  { role: 'assistant', content: 'Use nmap with the -sS flag...' },
  { role: 'user', content: 'What ports should I scan?' },
  # ... test conversation continuation
]
```

### Performance Benchmarks

**Target Metrics** (from Epic 2):
- Cache precomputation time: ≤5 minutes for typical knowledge base
- Cache loading time: ≤30 seconds from stored cache
- Query response time: ≤2 seconds (excluding LLM inference)
- Memory usage: ≤6GB for typical knowledge bases with CAG
- Latency improvement over RAG: 50-80% target

[Source: docs/stories/epic-2-cag-system-implementation.md]

### CAG vs RAG Comparison Points

**Key Differences to Test**:
- CAG preloads all knowledge vs RAG retrieves on-demand
- CAG uses KV cache vs RAG uses vector search
- CAG targets lower latency through pre-computation
- CAG requires more memory but eliminates search overhead
- Both should support offline operation

**Comparison Tests**:
```ruby
def test_cag_vs_rag_latency
  # Run same query through both systems
  cag_start = Time.now
  cag_result = @cag_manager.query("What is credential dumping?")
  cag_time = Time.now - cag_start

  rag_start = Time.now
  rag_result = @rag_manager.query("What is credential dumping?")
  rag_time = Time.now - rag_start

  # Verify CAG is significantly faster
  assert cag_time < rag_time * 0.5, "CAG should be 50%+ faster than RAG"
end
```

### Offline Testing

**CRITICAL**: Tests must pass without external network access

- Use Ollama for local long-context model (or mock LLM client)
- Use local cache storage (temporary directories)
- No external API calls in core tests
- Network tests should be optional/skipped in offline mode

[Source: docs/prd.md#2.3 CR8: Offline Operation Compatibility]

### Error Handling Pattern

```ruby
def test_error_scenario
  assert_raises(CAGError) do
    @cag_manager.method_that_should_fail
  end
end

def test_graceful_degradation
  result = @cag_manager.query_with_missing_cache
  assert_nil result  # Should return nil, not crash
end

def test_cache_corruption_handling
  # Corrupt cache file
  cache_file = File.join(@test_cache_dir, 'test_cache.bin')
  File.write(cache_file, 'corrupted_data')

  # Should handle gracefully
  assert_nothing_raised do
    @cag_manager.load_cache
  end
end
```

### Coverage Measurement

If using SimpleCov:
```ruby
# In test_helper.rb or top of test file
require 'simplecov'
SimpleCov.start do
  add_filter '/test/'
  add_group 'CAG System', 'cag/'
  add_group 'RAG System', 'rag/'
end
```

### Cache Testing Utilities

```ruby
# Helper methods for cache testing
def create_test_cache(documents)
  loader = @cag_manager.knowledge_loader
  loader.load_documents(documents)

  cache_mgr = @cag_manager.cache_manager
  cache_mgr.precompute_cache
end

def verify_cache_integrity(cache_path)
  assert File.exist?(cache_path), "Cache file should exist"
  assert File.size(cache_path) > 0, "Cache file should not be empty"

  # Verify cache can be loaded
  cache_data = @cag_manager.cache_manager.load_cache(cache_path)
  refute_nil cache_data, "Cache should load successfully"
end

def measure_cache_size(cache_path)
  File.size(cache_path) / (1024.0 * 1024.0)  # Size in MB
end
```

### Test Data Organization

```
test/fixtures/cag_test_data/
├── small_kb/           # Small knowledge base (10-20 docs)
│   ├── doc1.md
│   ├── doc2.md
│   └── ...
├── medium_kb/          # Medium knowledge base (100-200 docs)
│   └── ...
├── large_kb/           # Large knowledge base (1000+ docs)
│   └── ...
└── edge_cases/         # Edge case test documents
    ├── empty.md
    ├── malformed.md
    ├── huge_doc.md     # Tests context window limits
    └── ...
```

---

## Testing

### Testing Strategy for This Story

**This IS the testing story** - we're creating tests, not testing tests.

**Validation Approach**:
1. **Manual Verification**: Run test suite, verify all pass
2. **Coverage Check**: Measure code coverage, verify ≥80%
3. **Performance Check**: Measure test execution time, verify ≤10 minutes
4. **Offline Check**: Run tests without network, verify pass
5. **RAG Coexistence Check**: Run both CAG and RAG tests, verify no conflicts

### Test Execution Commands

```bash
# Run CAG comprehensive tests
ruby test/test_cag_comprehensive.rb

# Run with verbose output
ruby test/test_cag_comprehensive.rb --verbose

# Run in Nix environment
nix develop
ruby test/test_cag_comprehensive.rb

# Run all tests (CAG + RAG)
ruby test/run_tests.rb

# Run with coverage
COVERAGE=1 ruby test/test_cag_comprehensive.rb
```

### Success Criteria

✅ All tests pass
✅ ≥80% code coverage on CAG manager and components
✅ Tests complete in ≤10 minutes
✅ Tests pass offline
✅ No production data or caches modified
✅ No conflicts with RAG system tests
✅ All CAG components thoroughly validated

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | v1.0 | Initial story creation | SM Agent |

---

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

---

## QA Results

_This section will be populated by QA Agent after story completion._

---

**Story prepared by**: Scrum Master Agent
**Ready for**: Developer Agent implementation (after Story 2.6 completion)
**Next Story**: 2.8 - Performance Validation and Optimization (depends on this story completion)
