# Story 1.5: Implement RAG vs CAG Performance Comparison

**Epic**: Epic 1 - LLM Feature Stabilization
**Story ID**: 1.5
**Priority**: High
**Estimated Effort**: 3-4 days
**Dependencies**: Story 1.3 (RAG tests complete), Story 1.4 (CAG tests complete)

---

## Status

**Draft**

---

## Story

**As a** developer,
**I want** to quantitatively compare RAG and CAG performance across multiple dimensions,
**so that** I can make a data-driven architectural decision for production deployment.

---

## Acceptance Criteria

1. Test file created: test/test_rag_cag_performance.rb
2. Performance test suite includes ≥100 cybersecurity-focused queries
3. Metrics collected for both systems: query latency, memory usage, cache/index loading time
4. Result relevance evaluation performed for both systems
5. Statistical analysis performed on collected metrics (mean, median, percentiles, standard deviation)
6. Performance comparison report generated with charts/tables
7. Architectural recommendation documented based on quantitative findings
8. All tests pass in Nix development environment

---

## Integration Verification

- **IV1**: Verify performance tests don't modify production knowledge bases
- **IV2**: Verify tests can run unattended for full test suite execution
- **IV3**: Verify test results are reproducible across multiple runs

---

## Tasks / Subtasks

### Phase 1: Test Infrastructure Setup (AC: 1)

- [ ] **Task 1.1: Create Performance Test File**
  - [ ] Create test/test_rag_cag_performance.rb
  - [ ] Set up test framework (Minitest)
  - [ ] Create test class: `TestRAGCAGPerformance < Minitest::Test`
  - [ ] Add setup and teardown methods
  - [ ] Create results directory: test/results/

- [ ] **Task 1.2: Set Up Metrics Collection Framework**
  - [ ] Create metrics collection module/class
  - [ ] Implement timing measurement utilities
  - [ ] Implement memory usage measurement utilities
  - [ ] Create results storage structure (CSV or JSON)
  - [ ] Set up logging for test progress

- [ ] **Task 1.3: Configure Test Environments**
  - [ ] Set up RAG test configuration (same as Story 1.3)
  - [ ] Set up CAG test configuration (same as Story 1.4)
  - [ ] Ensure both use same knowledge base for fair comparison
  - [ ] Configure test to run both systems sequentially

### Phase 2: Query Set Creation (AC: 2)

- [ ] **Task 2.1: Design Query Categories**
  - [ ] Category 1: General cybersecurity concepts (25 queries)
  - [ ] Category 2: Specific tools and commands (25 queries)
  - [ ] Category 3: Attack techniques (MITRE ATT&CK) (25 queries)
  - [ ] Category 4: Defensive measures (25 queries)
  - [ ] Category 5: Complex multi-concept queries (10+ queries)

- [ ] **Task 2.2: Create Query Test Data**
  - [ ] Create test/fixtures/performance_queries.yaml (or .json)
  - [ ] Document expected result types for each query
  - [ ] Include query difficulty ratings (simple, medium, complex)
  - [ ] Include expected relevant documents for relevance scoring
  - [ ] Validate total query count ≥100

- [ ] **Task 2.3: Example Queries**
  ```yaml
  # Example structure
  queries:
    - query: "How do I scan for open ports?"
      category: "tools_commands"
      difficulty: "simple"
      expected_topics: ["nmap", "port scanning", "reconnaissance"]

    - query: "What is credential dumping and how can I detect it?"
      category: "attack_techniques"
      difficulty: "medium"
      expected_topics: ["T1003", "mimikatz", "credential access"]

    - query: "Explain the difference between symmetric and asymmetric encryption with examples"
      category: "general_concepts"
      difficulty: "complex"
      expected_topics: ["cryptography", "AES", "RSA"]
  ```

### Phase 3: Performance Metric Collection (AC: 3)

- [ ] **Task 3.1: Implement Query Latency Measurement**
  - [ ] Measure time from query submission to result retrieval
  - [ ] Record latency for each of 100+ queries
  - [ ] Exclude LLM inference time (measure RAG/CAG only)
  - [ ] Store per-query latency data
  - [ ] Calculate aggregate statistics

- [ ] **Task 3.2: Implement Memory Usage Measurement**
  - [ ] Measure baseline memory before loading knowledge base
  - [ ] Measure memory after knowledge base loaded
  - [ ] Measure peak memory during query processing
  - [ ] Record memory usage for both RAG and CAG
  - [ ] Use Ruby `ObjectSpace` or system tools for measurement

- [ ] **Task 3.3: Implement Cache/Index Loading Time Measurement**
  - [ ] Measure time to load knowledge base into RAG vector DB
  - [ ] Measure time to load knowledge base into CAG cache
  - [ ] Measure time for index building (if applicable)
  - [ ] Record loading times for various knowledge base sizes
  - [ ] Test with small (~100 docs), medium (~500 docs), large (~1000+ docs) knowledge bases

- [ ] **Task 3.4: Create Measurement Utilities**
  ```ruby
  class PerformanceMetrics
    def measure_latency(&block)
      start_time = Time.now
      result = block.call
      end_time = Time.now
      { result: result, latency_ms: (end_time - start_time) * 1000 }
    end

    def measure_memory(&block)
      GC.start
      before = get_memory_usage
      result = block.call
      after = get_memory_usage
      { result: result, memory_mb: (after - before) / 1024.0 / 1024.0 }
    end

    def get_memory_usage
      # Platform-specific memory measurement
    end
  end
  ```

### Phase 4: Result Relevance Evaluation (AC: 4)

- [ ] **Task 4.1: Design Relevance Scoring Method**
  - [ ] Define relevance criteria (exact match, partial match, no match)
  - [ ] Create rubric for scoring returned results
  - [ ] Determine scoring scale (e.g., 0-5 or 0-10)
  - [ ] Document scoring methodology

- [ ] **Task 4.2: Implement Relevance Scoring**
  - [ ] For each query, check if expected topics appear in results
  - [ ] Score based on ranking of relevant results (higher score if top-ranked)
  - [ ] Calculate precision@k (k=1,3,5,10)
  - [ ] Calculate recall if applicable
  - [ ] Store relevance scores per query

- [ ] **Task 4.3: Manual Validation Sample**
  - [ ] Manually review 10-20 query results from each system
  - [ ] Validate automated scoring accuracy
  - [ ] Document any edge cases or scoring issues
  - [ ] Adjust scoring algorithm if needed

### Phase 5: Statistical Analysis (AC: 5)

- [ ] **Task 5.1: Implement Statistical Calculations**
  - [ ] Calculate mean, median, mode for all metrics
  - [ ] Calculate standard deviation and variance
  - [ ] Calculate percentiles (p50, p90, p95, p99)
  - [ ] Identify outliers and analyze causes
  - [ ] Perform t-tests or other significance tests if appropriate

- [ ] **Task 5.2: Create Comparison Analysis**
  - [ ] Compare RAG vs CAG for each metric
  - [ ] Calculate percentage differences
  - [ ] Identify which system performs better in which categories
  - [ ] Analyze trade-offs (e.g., speed vs relevance)
  - [ ] Document findings

- [ ] **Task 5.3: Statistical Analysis Utilities**
  ```ruby
  class StatisticalAnalysis
    def calculate_stats(data_array)
      {
        mean: calculate_mean(data_array),
        median: calculate_median(data_array),
        std_dev: calculate_std_dev(data_array),
        percentiles: {
          p50: percentile(data_array, 50),
          p90: percentile(data_array, 90),
          p95: percentile(data_array, 95),
          p99: percentile(data_array, 99)
        },
        min: data_array.min,
        max: data_array.max
      }
    end
  end
  ```

### Phase 6: Performance Report Generation (AC: 6)

- [ ] **Task 6.1: Create Report Structure**
  - [ ] Create test/results/performance_report.md
  - [ ] Define report sections:
    - Executive Summary
    - Test Methodology
    - Query Latency Results
    - Memory Usage Results
    - Loading Time Results
    - Relevance Results
    - Statistical Comparison
    - Recommendations
  - [ ] Include timestamp and test environment details

- [ ] **Task 6.2: Generate Data Visualizations**
  - [ ] Create latency comparison chart (box plots or histograms)
  - [ ] Create memory usage comparison chart
  - [ ] Create loading time comparison chart
  - [ ] Create relevance score comparison chart
  - [ ] Use ASCII charts or export data for external visualization

- [ ] **Task 6.3: Create Summary Tables**
  - [ ] Latency summary table (mean, median, p95, p99 for both systems)
  - [ ] Memory usage summary table
  - [ ] Loading time summary table
  - [ ] Relevance score summary table
  - [ ] Overall comparison table with "winner" for each metric

- [ ] **Task 6.4: Example Report Section**
  ```markdown
  ## Query Latency Results

  | Metric | RAG | CAG | Winner |
  |--------|-----|-----|--------|
  | Mean | 1.2s | 0.8s | CAG |
  | Median | 1.0s | 0.7s | CAG |
  | P95 | 2.5s | 1.5s | CAG |
  | P99 | 3.2s | 2.0s | CAG |

  **Analysis**: CAG demonstrates 33% faster query latency on average...
  ```

### Phase 7: Architectural Recommendation (AC: 7)

- [ ] **Task 7.1: Analyze Trade-offs**
  - [ ] List advantages of RAG approach
  - [ ] List advantages of CAG approach
  - [ ] Identify use cases favoring each approach
  - [ ] Consider implementation complexity
  - [ ] Consider maintainability

- [ ] **Task 7.2: Make Data-Driven Recommendation**
  - [ ] Recommend: RAG-only, CAG-only, or Hybrid approach
  - [ ] Provide clear rationale based on performance data
  - [ ] Document any caveats or limitations
  - [ ] Suggest configuration recommendations
  - [ ] Identify optimization opportunities

- [ ] **Task 7.3: Document Decision**
  - [ ] Add recommendation section to performance report
  - [ ] Include supporting data references
  - [ ] Suggest next steps for implementation
  - [ ] Flag any blocking issues or concerns

- [ ] **Task 7.4: Example Recommendation**
  ```markdown
  ## Architectural Recommendation

  **Recommendation**: Proceed with RAG-only approach for production deployment.

  **Rationale**:
  - RAG demonstrates superior relevance scores (8.2/10 vs 6.5/10)
  - RAG latency acceptable for use case (<2s p95)
  - CAG implementation complexity outweighs speed benefits
  - RAG better handles dynamic knowledge base updates

  **Trade-offs Accepted**:
  - 40% slower query latency than CAG
  - Higher memory usage during operation

  **Next Steps**:
  - Optimize RAG vector similarity search
  - Implement caching for frequent queries
  - Proceed with SecGen integration using RAG
  ```

### Phase 8: Integration and Validation (AC: 8)

- [ ] **Task 8.1: Run Full Performance Test Suite**
  - [ ] Execute test/test_rag_cag_performance.rb
  - [ ] Verify all 100+ queries run successfully
  - [ ] Verify metrics collected for both systems
  - [ ] Verify report generated
  - [ ] Review for any errors or anomalies

- [ ] **Task 8.2: Reproducibility Testing**
  - [ ] Run test suite multiple times (3-5 runs)
  - [ ] Compare results across runs
  - [ ] Calculate variance between runs
  - [ ] Verify conclusions remain consistent
  - [ ] Document any reproducibility issues

- [ ] **Task 8.3: Integration Verification** (IV1, IV2, IV3)
  - [ ] **IV1**: Verify no production data modified
  - [ ] **IV2**: Verify tests run unattended without manual intervention
  - [ ] **IV3**: Verify results reproducible across runs
  - [ ] Document test execution time (may be >5 minutes for 100+ queries)

---

## Dev Notes

### Performance Testing Strategy

**Goal**: Provide quantitative data to inform architectural decision between RAG, CAG, or hybrid approach.

**Not a Benchmark**: This is a comparative analysis for internal decision-making, not a general-purpose benchmark.

**Fair Comparison Principles**:
- Same knowledge base for both systems
- Same query set for both systems
- Same hardware and environment
- Measure only RAG/CAG performance (exclude LLM inference)
- Isolate variables (test one thing at a time)

[Source: Epic 1 success criteria]

### Query Categories and Examples

**Category 1: General Cybersecurity Concepts**
```
- "What is the CIA triad?"
- "Explain the difference between authentication and authorization"
- "What are the phases of a penetration test?"
- "Define zero-day vulnerability"
```

**Category 2: Tools and Commands**
```
- "How do I use nmap to scan for vulnerabilities?"
- "What options does grep support?"
- "Explain how to use Wireshark for packet analysis"
- "How do I configure iptables firewall rules?"
```

**Category 3: Attack Techniques**
```
- "What is SQL injection and how does it work?"
- "Describe credential dumping techniques"
- "How does a man-in-the-middle attack work?"
- "Explain pass-the-hash attacks"
```

**Category 4: Defensive Measures**
```
- "How can I detect port scanning?"
- "What are best practices for secure password storage?"
- "How do I harden an SSH server?"
- "Describe intrusion detection systems"
```

**Category 5: Complex Multi-Concept**
```
- "Compare symmetric vs asymmetric encryption with use cases"
- "Explain the attack lifecycle from reconnaissance to exfiltration"
- "Describe defense in depth strategy with specific examples"
```

### Metrics Reference Values

**Target Performance** (from PRD NFRs):
- Query latency: ≤ 5 seconds (NFR4)
- Memory usage: ≤ 4GB for 1000+ documents (NFR2)
- Loading time: ≤ 60 seconds (NFR3)

**Comparison Goal**: Determine if either system meets requirements, and which performs better.

[Source: docs/prd.md#2.2 Non-Functional Requirements]

### Memory Measurement Approach

**Ruby Memory Measurement**:
```ruby
def get_memory_usage
  # Linux-specific
  if RUBY_PLATFORM =~ /linux/
    status = `cat /proc/#{Process.pid}/status`
    status.match(/VmRSS:\s+(\d+)/)[1].to_i  # KB
  else
    # Fallback: ObjectSpace (less accurate)
    ObjectSpace.count_objects[:TOTAL] * 100  # Rough estimate
  end
end
```

**System Tools Alternative**:
- Use `/usr/bin/time -v` wrapper
- Parse RSS and peak memory from output

### Relevance Scoring Rubric

**Scoring Scale** (0-10):
- **10**: Perfect match - all expected topics in top 3 results
- **8-9**: Good match - most expected topics in top 5 results
- **6-7**: Partial match - some expected topics in top 10 results
- **4-5**: Weak match - expected topics present but low-ranked
- **2-3**: Poor match - few expected topics, mostly irrelevant
- **0-1**: No match - no expected topics in results

**Precision@k**:
```ruby
def precision_at_k(results, expected_topics, k)
  top_k = results.take(k)
  relevant_count = top_k.count { |r| contains_expected_topic?(r, expected_topics) }
  relevant_count.to_f / k
end
```

### Statistical Significance

**Sample Size**: 100+ queries provides reasonable statistical power

**Significance Testing**:
- Use Welch's t-test for comparing means (if distributions roughly normal)
- Use Mann-Whitney U test for non-parametric comparison
- Report p-values and confidence intervals
- Document any assumptions made

**Practical Significance**:
- Don't just report statistical significance
- Report effect size (how big is the difference?)
- Consider practical impact for use case

### Report Output Formats

**Primary Report**: Markdown (test/results/performance_report.md)
- Human-readable
- Can be committed to repo
- Easy to review in pull requests

**Supporting Data**: CSV or JSON (test/results/performance_data.csv)
- Raw data for further analysis
- Can import into spreadsheet or R/Python if needed
- Enables reproducibility

**Visualizations**: ASCII charts in Markdown OR data export
- Simple ASCII charts using Ruby gems (e.g., `terminal-table`)
- OR export data for plotting with external tools

### Adaptation for RAG-Only Fallback

**IF Story 1.2 triggered RAG-only fallback**:
- This story becomes "RAG Performance Validation"
- Remove CAG comparison elements
- Focus on validating RAG meets performance requirements
- Compare against baseline/requirements, not against another system
- Still generate performance report, but different structure

[Source: Epic 1 rollback plan]

### Time Considerations

**Long-Running Test**: 100+ queries × 2 systems = 200+ query executions

**Estimated Time**:
- If average query takes 1-2 seconds: 3-7 minutes for queries only
- Plus loading time, setup, teardown: 10-15 minutes total
- May exceed 5-minute test suite target (acceptable for this story)

**Optimization**:
- Run RAG and CAG tests in parallel if possible
- Cache knowledge base loading between queries
- Provide progress indicators

### Existing System Context

**RAG Implementation**:
- Uses vector embeddings (ChromaDB)
- Ollama or OpenAI for embeddings
- Similarity search for retrieval
- [Source: docs/development/architecture.md#245-261]

**CAG Implementation** (varies based on Story 1.2):
- May use knowledge graph OR simplified cache
- In-memory storage
- Entity-based or keyword-based retrieval
- [Source: Story 1.2 implementation decision]

**Knowledge Sources**:
- MITRE ATT&CK techniques
- Man pages (command references)
- Markdown lab sheets
- [Source: docs/prd.md#1.3 Feature Requirements]

---

## Testing

### Testing Strategy for This Story

**Validation Approach**:
1. **Functionality Test**: Verify test suite runs to completion
2. **Data Quality**: Verify metrics collected are valid and reasonable
3. **Reproducibility**: Run multiple times, verify consistent conclusions
4. **Report Review**: Manually review generated report for completeness

### Test Execution

```bash
# Run performance comparison test
ruby test/test_rag_cag_performance.rb

# Run with verbose output
ruby test/test_rag_cag_performance.rb --verbose

# Run in Nix environment
nix develop
ruby test/test_rag_cag_performance.rb

# View results
cat test/results/performance_report.md
```

### Success Criteria

✅ Test suite runs successfully for 100+ queries
✅ Metrics collected for RAG and CAG (or RAG only if fallback)
✅ Statistical analysis completed
✅ Performance report generated with data and visualizations
✅ Architectural recommendation documented with rationale
✅ Results reproducible across multiple test runs

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-17 | v1.0 | Initial story creation | SM Agent (Bob) |

---

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

---

## QA Results

_This section will be populated by QA Agent after story completion._

---

**Story prepared by**: Scrum Master Agent (Bob)
**Ready for**: Developer Agent implementation (after Stories 1.3 and 1.4 complete)
**Next Story**: 1.6 - Document Findings and Architectural Recommendations (depends on this story)

**⚠️ CONDITIONAL STORY**: If RAG-only fallback triggered, this becomes "RAG Performance Validation" instead of comparison study. Adapt tasks accordingly.
