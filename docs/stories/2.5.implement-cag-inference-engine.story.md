# Story 2.5: Implement CAG Inference Engine

**Epic**: Epic 2 - CAG System Implementation
**Story ID**: 2.5
**Priority**: Critical
**Estimated Effort**: 3-4 days
**Dependencies**: Story 2.4 (Cache Manager)

---

## Status

**Not Started**

---

## Story

**As a** developer,
**I want** to implement the CAG inference engine for response generation using cached context,
**so that** I can achieve optimal query performance (≤2 seconds) with 50-80% improvement over RAG.

---

## Acceptance Criteria

1. Implementation file created: cag/inference_engine.rb
2. Query processing pipeline implemented (input validation, preprocessing, context injection)
3. Context injection from cached KV cache working correctly
4. Response generation using cached context functional
5. Query response time ≤2 seconds (50-80% improvement over RAG baseline)
6. Multi-turn conversation support with context continuity
7. Error handling for cache unavailability and LLM failures
8. Performance optimization for minimal latency overhead
9. Integration tests pass with cache manager and LLM clients
10. Test coverage ≥80% for inference_engine.rb

---

## Integration Verification

- **IV1**: Verify inference engine works with all LLM providers (Ollama, OpenAI, VLLM, SGLang)
- **IV2**: Verify query response times meet ≤2 second target for typical cybersecurity queries
- **IV3**: Verify cached context properly injected without truncation or corruption
- **IV4**: Verify multi-turn conversations maintain context coherence
- **IV5**: Verify graceful degradation when cache unavailable

---

## Tasks / Subtasks

### Phase 1: Core Inference Engine Setup (AC: 1, 2)

- [ ] **Task 1.1: Create InferenceEngine Class Structure**
  - [ ] Create cag/inference_engine.rb file
  - [ ] Examine existing LLM client interfaces (llm_client.rb, llm_client_factory.rb)
  - [ ] Examine existing RAG inference patterns (rag/rag_manager.rb)
  - [ ] Define InferenceEngine class with initialization
  - [ ] Add configuration parameters (model, temperature, max_tokens, etc.)
  - [ ] Create instance variables for cache manager, LLM client references

- [ ] **Task 1.2: Implement Query Preprocessing**
  - [ ] Create query_preprocess(query) method
  - [ ] Validate query input (nil, empty, length checks)
  - [ ] Sanitize query text (remove control characters, normalize whitespace)
  - [ ] Extract conversation context from multi-turn queries
  - [ ] Normalize cybersecurity-specific terms for consistency
  - [ ] Add error handling for invalid queries

- [ ] **Task 1.3: Implement Query Processing Pipeline**
  - [ ] Create process_query(query, options) main entry point
  - [ ] Call query_preprocess for input validation
  - [ ] Prepare query metadata (timestamp, user context, conversation ID)
  - [ ] Build processing context hash with query and metadata
  - [ ] Add logging with Print.debug for query processing steps
  - [ ] Return structured query context for next phase

### Phase 2: Context Injection Implementation (AC: 3, 4)

- [ ] **Task 2.1: Implement Cache Context Retrieval**
  - [ ] Create get_cached_context(query_context) method
  - [ ] Interface with cache_manager to retrieve KV cache
  - [ ] Verify cache availability and validity
  - [ ] Handle cache miss scenarios gracefully
  - [ ] Measure cache retrieval latency
  - [ ] Log cache hit/miss with Print.debug

- [ ] **Task 2.2: Implement Context Injection Strategy**
  - [ ] Create inject_context(query, cached_context) method
  - [ ] Format system prompt with cached knowledge base context
  - [ ] Inject user query into prompt structure
  - [ ] Handle context window limits for LLM model
  - [ ] Implement context prioritization if truncation needed
  - [ ] Verify context coherence after injection

- [ ] **Task 2.3: Implement LLM Prompt Assembly**
  - [ ] Create assemble_prompt(query_context, cached_context) method
  - [ ] Build complete prompt with system context + cached knowledge + user query
  - [ ] Format prompt according to model-specific requirements
  - [ ] Add conversation history for multi-turn scenarios
  - [ ] Optimize prompt structure for performance
  - [ ] Validate prompt length within model limits

### Phase 3: Response Generation Implementation (AC: 4, 5)

- [ ] **Task 3.1: Implement LLM Response Generation**
  - [ ] Create generate_response(prompt, options) method
  - [ ] Interface with LLM client (via llm_client_factory.rb)
  - [ ] Send prompt with appropriate generation parameters
  - [ ] Handle streaming vs non-streaming responses
  - [ ] Measure LLM inference latency
  - [ ] Log generation metrics with Print.debug

- [ ] **Task 3.2: Implement Response Post-Processing**
  - [ ] Create postprocess_response(response) method
  - [ ] Clean up response formatting (whitespace, newlines)
  - [ ] Extract answer from response structure
  - [ ] Validate response quality (non-empty, coherent)
  - [ ] Add response metadata (latency, cache hit, model used)
  - [ ] Return structured response object

- [ ] **Task 3.3: Implement End-to-End Query Method**
  - [ ] Create query(user_query, options = {}) main public method
  - [ ] Combine all pipeline steps: preprocess -> context -> generate -> postprocess
  - [ ] Measure total query latency (target: ≤2 seconds)
  - [ ] Return response with metadata (answer, latency, cache status)
  - [ ] Add comprehensive error handling for entire pipeline
  - [ ] Log complete query flow with Print.info

### Phase 4: Performance Optimization (AC: 5, 8)

- [ ] **Task 4.1: Implement Latency Profiling**
  - [ ] Add timing instrumentation to all pipeline stages
  - [ ] Measure: preprocessing, cache retrieval, context injection, LLM inference, postprocessing
  - [ ] Identify bottlenecks in query processing pipeline
  - [ ] Log detailed timing breakdown with Print.debug
  - [ ] Create performance metrics reporting

- [ ] **Task 4.2: Optimize Critical Path Performance**
  - [ ] Minimize string concatenation overhead (use array join)
  - [ ] Optimize context injection to reduce memory allocation
  - [ ] Cache prompt templates to avoid repeated formatting
  - [ ] Parallelize independent operations where possible
  - [ ] Reduce redundant validation checks in hot path
  - [ ] Profile and optimize JSON parsing/serialization

- [ ] **Task 4.3: Validate Performance Targets**
  - [ ] Test query latency with typical cybersecurity queries
  - [ ] Verify ≤2 second total response time (excluding LLM inference)
  - [ ] Compare with RAG baseline (validate 50-80% improvement)
  - [ ] Test with different LLM providers (Ollama, OpenAI)
  - [ ] Measure performance with various cache sizes
  - [ ] Document achieved latency metrics

### Phase 5: Multi-Turn Conversation Support (AC: 6)

- [ ] **Task 5.1: Implement Conversation Context Tracking**
  - [ ] Create conversation_history instance variable
  - [ ] Add update_conversation_history(query, response) method
  - [ ] Store conversation turns with timestamps
  - [ ] Implement conversation history size limits
  - [ ] Add conversation reset functionality
  - [ ] Track conversation ID for multi-user scenarios

- [ ] **Task 5.2: Implement Context Continuity**
  - [ ] Modify assemble_prompt to include conversation history
  - [ ] Format multi-turn context for LLM consumption
  - [ ] Handle conversation history truncation for context limits
  - [ ] Preserve important conversation context during truncation
  - [ ] Test context coherence across multiple query turns
  - [ ] Verify conversation state management

- [ ] **Task 5.3: Test Multi-Turn Scenarios**
  - [ ] Create test conversation flows (2-5 turns)
  - [ ] Test follow-up questions maintaining context
  - [ ] Test context switches and topic changes
  - [ ] Verify conversation history doesn't cause context overflow
  - [ ] Test conversation reset and new conversation starts
  - [ ] Validate multi-user conversation isolation

### Phase 6: Error Handling and Resilience (AC: 7)

- [ ] **Task 6.1: Implement Cache Unavailability Handling**
  - [ ] Detect cache miss or unavailability errors
  - [ ] Implement fallback strategy (empty context or RAG fallback)
  - [ ] Log cache unavailability with Print.err
  - [ ] Return meaningful error response to user
  - [ ] Track cache miss rate for monitoring
  - [ ] Test cache failure scenarios

- [ ] **Task 6.2: Implement LLM Failure Handling**
  - [ ] Handle LLM client connection failures
  - [ ] Handle LLM generation timeouts
  - [ ] Handle LLM API errors (rate limits, invalid responses)
  - [ ] Implement retry logic with exponential backoff
  - [ ] Log LLM failures with Print.err
  - [ ] Return graceful error messages to users

- [ ] **Task 6.3: Implement Comprehensive Error Recovery**
  - [ ] Handle context injection errors (truncation, formatting)
  - [ ] Handle prompt assembly errors (invalid format, size limits)
  - [ ] Handle response parsing errors (malformed JSON, empty responses)
  - [ ] Add error context to all exceptions (query, stage, state)
  - [ ] Test all error scenarios with edge cases
  - [ ] Verify system never crashes on errors

### Phase 7: Testing and Integration (AC: 9, 10)

- [ ] **Task 7.1: Create Unit Tests for Inference Engine**
  - [ ] Create test/test_cag_inference_engine.rb
  - [ ] Test query preprocessing with various inputs
  - [ ] Test context injection with mock cache
  - [ ] Test prompt assembly with different scenarios
  - [ ] Test response generation with mock LLM
  - [ ] Test response postprocessing
  - [ ] Test error handling for all failure modes

- [ ] **Task 7.2: Create Integration Tests**
  - [ ] Test integration with cache_manager (real cache)
  - [ ] Test integration with LLM clients (Ollama, OpenAI)
  - [ ] Test end-to-end query flow with real components
  - [ ] Test multi-turn conversations with real LLM
  - [ ] Test performance with actual cybersecurity queries
  - [ ] Test all LLM provider integrations

- [ ] **Task 7.3: Measure Test Coverage**
  - [ ] Run tests with coverage tool (SimpleCov)
  - [ ] Verify ≥80% coverage for cag/inference_engine.rb
  - [ ] Identify untested code paths
  - [ ] Add tests for missing coverage
  - [ ] Verify all public methods tested
  - [ ] Verify all error paths tested

- [ ] **Task 7.4: Integration Verification** (IV1-IV5)
  - [ ] **IV1**: Test with all LLM providers (Ollama, OpenAI, VLLM, SGLang)
  - [ ] **IV2**: Measure and verify ≤2 second query response times
  - [ ] **IV3**: Verify cached context injection integrity
  - [ ] **IV4**: Test multi-turn conversation coherence
  - [ ] **IV5**: Test graceful degradation without cache
  - [ ] Document integration test results

---

## Dev Notes

### CAG Inference Engine Architecture

**InferenceEngine** (`cag/inference_engine.rb`):
- Main coordinator for CAG query processing and response generation
- Interfaces with: cache_manager (context), LLM clients (generation)
- Query pipeline: preprocess -> retrieve cache -> inject context -> generate -> postprocess
- Performance target: ≤2 seconds total latency (50-80% improvement over RAG)
- [Source: docs/stories/epic-2-cag-system-implementation.md#148-156]

**Performance Requirements**:
- Query response time: ≤2 seconds (excluding LLM inference time)
- 50-80% latency improvement over RAG baseline (RAG: ≤5 seconds)
- Memory usage: ≤6GB for typical knowledge bases with KV caches
- Cache loading time: ≤30 seconds from stored cache
- [Source: docs/prd.md#2.2, epic-2-cag-system-implementation.md#220-226]

### CAG Query Processing Pipeline

**Pipeline Stages**:
1. **Query Preprocessing**: Input validation, sanitization, normalization
2. **Cache Context Retrieval**: Fetch KV cache from cache_manager
3. **Context Injection**: Combine cached knowledge with user query
4. **Prompt Assembly**: Format complete prompt for LLM
5. **LLM Generation**: Call LLM client to generate response
6. **Response Postprocessing**: Clean and format response

**Latency Budget** (target: ≤2 seconds total):
- Preprocessing: ≤50ms
- Cache retrieval: ≤200ms
- Context injection: ≤100ms
- Prompt assembly: ≤50ms
- LLM inference: ~1-1.5 seconds (model-dependent, excluded from ≤2s target)
- Postprocessing: ≤100ms

### LLM Client Integration

**LLM Client Factory** (`llm_client_factory.rb`):
- Factory pattern for creating LLM client instances
- Supported providers: Ollama, OpenAI, VLLM, SGLang
- [Source: Codebase inspection]

**LLM Client Interface** (`llm_client.rb`):
- Abstract base class for LLM clients
- Methods: generate(prompt, options), stream(prompt, options)
- Common parameters: temperature, max_tokens, stop_sequences
- [Source: Codebase inspection]

**Integration Pattern**:
```ruby
class InferenceEngine
  def initialize(config)
    @cache_manager = config[:cache_manager]
    @llm_client = LLMClientFactory.create(config[:llm_provider], config[:llm_options])
  end

  def query(user_query, options = {})
    # Pipeline implementation
  end
end
```

### Cache Manager Integration

**CacheManager Interface** (`cag/cache_manager.rb`):
- Methods expected by inference engine:
  - get_cache(cache_id) - Retrieve KV cache for knowledge base
  - cache_available?(cache_id) - Check cache availability
  - get_cache_metadata(cache_id) - Get cache info (size, timestamp, etc.)
- [Source: Story 2.4 - Implement Cache Manager]

**Context Retrieval Pattern**:
```ruby
def get_cached_context(query_context)
  cache_id = determine_cache_id(query_context)

  unless @cache_manager.cache_available?(cache_id)
    Print.err("Cache unavailable: #{cache_id}")
    return nil
  end

  cache = @cache_manager.get_cache(cache_id)
  cache
rescue => e
  Print.err("Cache retrieval failed: #{e.message}")
  nil
end
```

### Context Injection Strategy

**System Prompt Template**:
```
You are a cybersecurity training assistant. You have access to comprehensive knowledge about:
- MITRE ATT&CK tactics and techniques
- Linux/Unix command reference
- Cybersecurity lab procedures

[CACHED_KNOWLEDGE_CONTEXT]

Answer the user's question using the provided knowledge base.
```

**Context Injection**:
```ruby
def inject_context(query, cached_context)
  system_prompt = build_system_prompt(cached_context)
  user_prompt = format_user_query(query)

  {
    system: system_prompt,
    user: user_prompt,
    conversation_history: @conversation_history
  }
end
```

### Multi-Turn Conversation Management

**Conversation History Structure**:
```ruby
@conversation_history = [
  { role: 'user', content: 'What is credential dumping?', timestamp: Time.now },
  { role: 'assistant', content: '...', timestamp: Time.now },
  { role: 'user', content: 'How do I detect it?', timestamp: Time.now },
  # ...
]
```

**Context Window Management**:
- Track total context size (cached knowledge + conversation history + current query)
- Implement intelligent truncation to stay within model context limits
- Preserve most recent conversation turns for continuity
- Prioritize cached knowledge over old conversation history

### Performance Optimization Techniques

**String Handling**:
```ruby
# Inefficient
prompt = ""
prompt += system_context
prompt += "\n\n"
prompt += user_query

# Efficient
prompt = [system_context, user_query].join("\n\n")
```

**Caching Compiled Templates**:
```ruby
def initialize(config)
  # Cache prompt templates
  @system_prompt_template = compile_system_prompt_template
  @user_prompt_template = compile_user_prompt_template
end
```

**Minimize Allocations**:
- Reuse buffers and strings where possible
- Avoid creating temporary objects in hot paths
- Use string mutation (<<) for incremental building when appropriate

### Error Handling Patterns

**Graceful Degradation**:
```ruby
def query(user_query, options = {})
  begin
    query_context = preprocess_query(user_query)
    cached_context = get_cached_context(query_context)

    # Fallback to empty context if cache unavailable
    cached_context ||= {}

    prompt = assemble_prompt(query_context, cached_context)
    response = generate_response(prompt, options)
    postprocess_response(response)
  rescue => e
    Print.err("Query processing failed: #{e.message}")
    Print.err(e.backtrace.join("\n"))
    { error: "Failed to process query", details: e.message }
  end
end
```

**Retry Logic**:
```ruby
def generate_response(prompt, options = {})
  max_retries = 3
  retry_count = 0

  begin
    @llm_client.generate(prompt, options)
  rescue LLMClient::TimeoutError, LLMClient::ConnectionError => e
    retry_count += 1
    if retry_count <= max_retries
      sleep(2 ** retry_count) # Exponential backoff
      retry
    else
      raise
    end
  end
end
```

### Testing Strategy

**Unit Tests** (`test/test_cag_inference_engine.rb`):
```ruby
require_relative 'test_helper'
require_relative '../cag/inference_engine'

class TestCAGInferenceEngine < Minitest::Test
  def setup
    @mock_cache_manager = MockCacheManager.new
    @mock_llm_client = MockLLMClient.new

    @inference_engine = InferenceEngine.new(
      cache_manager: @mock_cache_manager,
      llm_client: @mock_llm_client
    )
  end

  def test_query_preprocessing
    query = "  What is credential dumping?  \n"
    result = @inference_engine.send(:preprocess_query, query)
    assert_equal "What is credential dumping?", result[:normalized_query]
  end

  def test_query_with_cache_available
    @mock_cache_manager.set_cache_available(true)
    result = @inference_engine.query("Test query")
    assert result[:success]
    assert result[:response].length > 0
  end

  def test_query_with_cache_unavailable
    @mock_cache_manager.set_cache_available(false)
    result = @inference_engine.query("Test query")
    # Should still work with empty context
    assert result[:success]
  end

  # ... more tests
end
```

**Integration Tests**:
```ruby
def test_integration_with_real_cache_and_llm
  # Use real cache_manager and LLM client (Ollama)
  cache_manager = CacheManager.new(cache_config)
  llm_client = LLMClientFactory.create('ollama', ollama_config)

  inference_engine = InferenceEngine.new(
    cache_manager: cache_manager,
    llm_client: llm_client
  )

  result = inference_engine.query("What is T1003 in MITRE ATT&CK?")

  assert result[:success]
  assert result[:response].include?("Credential Dumping")
  assert result[:latency] <= 2.0, "Query exceeded 2 second target: #{result[:latency]}s"
end
```

**Performance Tests**:
```ruby
def test_query_latency_target
  queries = load_test_queries('test/fixtures/cybersecurity_queries.json')

  latencies = queries.map do |query|
    start_time = Time.now
    result = @inference_engine.query(query)
    Time.now - start_time
  end

  avg_latency = latencies.sum / latencies.size
  max_latency = latencies.max

  assert avg_latency <= 2.0, "Average latency exceeded 2s: #{avg_latency}s"
  assert max_latency <= 3.0, "Max latency exceeded 3s: #{max_latency}s"

  Print.info("Query latency - avg: #{avg_latency}s, max: #{max_latency}s")
end
```

### Cybersecurity Query Examples

**Test Queries for Performance Validation**:
```ruby
test_queries = [
  "What is credential dumping?",
  "How do I use nmap for port scanning?",
  "Explain MITRE ATT&CK technique T1003",
  "What are the common privilege escalation techniques?",
  "How do I detect lateral movement in a network?",
  "What is the purpose of the mimikatz tool?",
  "Explain the difference between TCP and UDP",
  "How do I use tcpdump to capture network traffic?",
  "What are indicators of compromise for ransomware?",
  "Describe the Cyber Kill Chain methodology"
]
```

### RAG Comparison Baseline

**RAG Performance Metrics** (from Epic 1):
- Query response time: ≤5 seconds (target)
- Average latency: ~3-4 seconds (estimated)
- Memory usage: ≤4GB

**CAG Target Performance**:
- Query response time: ≤2 seconds (60% improvement)
- Average latency: ~1-1.5 seconds (target)
- Memory usage: ≤6GB (allows for cache overhead)

**Comparison Methodology**:
- Use same query set for both RAG and CAG
- Measure total latency excluding LLM inference time
- Compare relevance and quality of responses
- Document performance characteristics and trade-offs

---

## Testing

### Testing Strategy for This Story

**Unit Testing**:
1. Test all pipeline stages independently with mocks
2. Test error handling for each failure mode
3. Test edge cases (empty queries, long queries, special characters)

**Integration Testing**:
1. Test with real cache_manager and LLM clients
2. Test all LLM provider integrations (Ollama, OpenAI, VLLM, SGLang)
3. Test end-to-end query flow with actual cybersecurity queries

**Performance Testing**:
1. Measure latency for all pipeline stages
2. Validate ≤2 second total query response time
3. Compare with RAG baseline for performance improvement validation

**Multi-Turn Testing**:
1. Test conversation continuity across 2-5 turns
2. Test context window management with long conversations
3. Test conversation state isolation for multiple users

### Test Execution Commands

```bash
# Run inference engine unit tests
ruby test/test_cag_inference_engine.rb

# Run with verbose output
ruby test/test_cag_inference_engine.rb --verbose

# Run integration tests (requires cache_manager and LLM)
ruby test/test_cag_integration.rb

# Run performance tests
ruby test/test_cag_performance.rb

# Measure test coverage
ruby -r simplecov test/test_cag_inference_engine.rb
```

### Success Criteria

✅ All unit tests pass
✅ All integration tests pass with all LLM providers
✅ Query response time ≤2 seconds (verified)
✅ 50-80% improvement over RAG baseline (verified)
✅ ≥80% test coverage for inference_engine.rb
✅ Multi-turn conversations work correctly
✅ Error handling graceful for all failure scenarios

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | v1.0 | Initial story creation for CAG inference engine | SM Agent |

---

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

---

## QA Results

_This section will be populated by QA Agent after story completion._

---

**Story prepared by**: Scrum Master Agent
**Ready for**: Developer Agent implementation (depends on Story 2.4 completion)
**Next Story**: 2.6 - Implement CAG Manager Coordinator (depends on this story completion)
