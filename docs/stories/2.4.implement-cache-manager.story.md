# Story 2.4: Implement Cache Manager

**Epic**: Epic 2 - CAG System Implementation
**Story ID**: 2.4
**Priority**: Critical
**Estimated Effort**: 4-5 days
**Dependencies**: Story 2.3 (Context Manager complete)

---

## Status

**Draft**

---

## Story

**As a** developer,
**I want** to implement the cache manager for KV cache precomputation, storage, and retrieval,
**so that** the CAG system can efficiently manage and utilize cached context for fast query responses.

---

## Acceptance Criteria

1. File created: cag/cache_manager.rb implementing CacheManager class
2. KV cache precomputation functionality implemented for long-context LLM models
3. Cache persistence implemented with binary storage format for efficiency
4. Cache loading and retrieval functionality implemented with <30s load time
5. Memory-efficient cache representation implemented to meet memory constraints
6. Cache invalidation strategies implemented for knowledge base updates
7. Cache versioning system implemented for compatibility management
8. Incremental cache update support implemented (add documents without full recompute)
9. Error handling implemented for cache corruption, missing files, version mismatches
10. Comprehensive unit tests created with >80% coverage for cache_manager.rb
11. All tests pass in Nix development environment

---

## Integration Verification

- **IV1**: Verify cache files stored in appropriate directory with correct permissions
- **IV2**: Verify cache loading doesn't interfere with normal bot operation
- **IV3**: Verify cache operations support offline mode (no external dependencies)
- **IV4**: Verify memory usage stays within NFR8 constraints (≤6GB)
- **IV5**: Verify cache precomputation completes within NFR9 time limit (≤5 minutes)

---

## Tasks / Subtasks

### Phase 1: Cache Manager Foundation (AC: 1, 2)

- [ ] **Task 1.1: Create CacheManager Class Structure**
  - [ ] Create cag/cache_manager.rb file
  - [ ] Define CacheManager class with initialization
  - [ ] Define cache storage paths and file naming conventions
  - [ ] Implement configuration handling (cache directory, compression settings, version)
  - [ ] Create cache metadata structure (version, timestamp, model info, document count)
  - [ ] Add logging for cache operations using Print utility

- [ ] **Task 1.2: Design Cache Storage Format**
  - [ ] Research and select binary storage format (MessagePack, CBOR, or custom)
  - [ ] Define cache file structure: header + metadata + KV cache data
  - [ ] Design compression strategy (optional zlib/gzip for large caches)
  - [ ] Define cache versioning scheme (semantic versioning)
  - [ ] Document storage format specification in code comments
  - [ ] Create cache file naming convention (e.g., cache_v1.0_model_hash.bin)

- [ ] **Task 1.3: Implement KV Cache Precomputation**
  - [ ] Implement `precompute_cache(context, model_info)` method
  - [ ] Integrate with LLM client to obtain KV cache from context processing
  - [ ] Handle different LLM provider cache formats (Ollama, OpenAI, VLLM, SGLang)
  - [ ] Extract and serialize KV cache data from model response
  - [ ] Implement progress tracking for long precomputation operations
  - [ ] Add timeout handling for precomputation (max 5 minutes per NFR9)
  - [ ] Implement error recovery for precomputation failures

### Phase 2: Cache Persistence and Storage (AC: 3, 5, 7)

- [ ] **Task 2.1: Implement Cache Serialization**
  - [ ] Implement `serialize_cache(cache_data, metadata)` method
  - [ ] Convert KV cache data to binary format
  - [ ] Add metadata header with version, timestamp, model info
  - [ ] Implement optional compression for large cache data
  - [ ] Add checksum/hash for cache integrity verification
  - [ ] Optimize serialization for memory efficiency

- [ ] **Task 2.2: Implement Cache Persistence**
  - [ ] Implement `save_cache(cache_data, cache_path)` method
  - [ ] Write serialized cache to disk with atomic operations
  - [ ] Implement file locking to prevent concurrent write corruption
  - [ ] Create cache directory structure if not exists
  - [ ] Set appropriate file permissions for cache files
  - [ ] Implement backup/rollback for failed writes
  - [ ] Add logging for cache save operations

- [ ] **Task 2.3: Implement Cache Versioning**
  - [ ] Define cache version compatibility matrix
  - [ ] Implement `check_cache_version(cache_metadata)` method
  - [ ] Handle version upgrades (migrate old cache to new format)
  - [ ] Handle version downgrades (reject incompatible caches)
  - [ ] Implement cache version migration utilities if needed
  - [ ] Document version compatibility in code and user docs

### Phase 3: Cache Loading and Retrieval (AC: 4, 5)

- [ ] **Task 3.1: Implement Cache Deserialization**
  - [ ] Implement `deserialize_cache(cache_data)` method
  - [ ] Parse binary cache format to extract metadata and KV data
  - [ ] Implement decompression if cache is compressed
  - [ ] Verify checksum/hash for cache integrity
  - [ ] Handle malformed or corrupted cache data gracefully
  - [ ] Optimize deserialization for memory efficiency

- [ ] **Task 3.2: Implement Cache Loading**
  - [ ] Implement `load_cache(cache_path)` method
  - [ ] Read cache file from disk with streaming for large files
  - [ ] Deserialize cache and extract metadata
  - [ ] Verify cache version compatibility
  - [ ] Implement memory-mapped loading for large caches (optional optimization)
  - [ ] Add progress tracking for cache loading
  - [ ] Ensure cache loading completes in <30s (target from Epic description)

- [ ] **Task 3.3: Implement Cache Retrieval Interface**
  - [ ] Implement `get_cache(cache_id)` method for retrieving cached context
  - [ ] Implement `list_caches()` method to enumerate available caches
  - [ ] Implement `get_cache_metadata(cache_id)` for cache inspection
  - [ ] Add caching layer for frequently accessed caches (in-memory)
  - [ ] Implement cache preloading on system startup (optional)

### Phase 4: Cache Invalidation and Updates (AC: 6, 8)

- [ ] **Task 4.1: Implement Cache Invalidation Strategies**
  - [ ] Implement `invalidate_cache(cache_id)` method
  - [ ] Implement time-based invalidation (cache expiration)
  - [ ] Implement content-based invalidation (knowledge base hash comparison)
  - [ ] Implement manual invalidation (user-triggered)
  - [ ] Add cache invalidation logging and notifications
  - [ ] Implement cache cleanup for invalidated/expired caches

- [ ] **Task 4.2: Implement Incremental Cache Updates**
  - [ ] Implement `update_cache(cache_id, new_documents)` method
  - [ ] Load existing cache and extract current context
  - [ ] Append new documents to context
  - [ ] Recompute KV cache for updated context (incremental if possible)
  - [ ] Save updated cache with new version/timestamp
  - [ ] Verify memory efficiency during incremental updates
  - [ ] Document incremental update limitations (when full recompute needed)

- [ ] **Task 4.3: Implement Knowledge Base Change Detection**
  - [ ] Implement `detect_knowledge_base_changes()` method
  - [ ] Calculate hash/fingerprint of knowledge base content
  - [ ] Compare with cached knowledge base fingerprint
  - [ ] Return list of added/modified/deleted documents
  - [ ] Determine if incremental update possible or full recompute needed
  - [ ] Cache knowledge base fingerprint for fast comparison

### Phase 5: Error Handling and Resilience (AC: 9)

- [ ] **Task 5.1: Implement Cache Corruption Handling**
  - [ ] Detect corrupted cache files (checksum mismatch)
  - [ ] Implement fallback to cache regeneration on corruption
  - [ ] Add detailed error logging for corruption cases
  - [ ] Implement cache repair utilities if feasible
  - [ ] Provide user-friendly error messages
  - [ ] Test with intentionally corrupted cache files

- [ ] **Task 5.2: Implement Missing Cache Handling**
  - [ ] Detect missing cache files gracefully
  - [ ] Implement automatic cache generation on first use
  - [ ] Provide progress feedback during cache generation
  - [ ] Handle missing cache directory creation
  - [ ] Test with various missing cache scenarios

- [ ] **Task 5.3: Implement Version Mismatch Handling**
  - [ ] Detect cache version incompatibilities
  - [ ] Implement automatic cache regeneration for incompatible versions
  - [ ] Provide warnings for version migrations
  - [ ] Log version mismatch events
  - [ ] Test with caches from different versions

- [ ] **Task 5.4: Implement General Error Handling**
  - [ ] Handle disk I/O errors (permissions, disk full, etc.)
  - [ ] Handle memory allocation failures during cache operations
  - [ ] Implement graceful degradation (fallback to no cache)
  - [ ] Add comprehensive error logging with Print.err
  - [ ] Test error scenarios: disk full, no permissions, OOM

### Phase 6: Memory Optimization (AC: 5)

- [ ] **Task 6.1: Implement Memory-Efficient Cache Representation**
  - [ ] Profile memory usage during cache operations
  - [ ] Implement streaming deserialization for large caches
  - [ ] Use memory-efficient data structures (arrays vs hashes)
  - [ ] Implement cache data lazy loading (load on demand)
  - [ ] Add memory usage monitoring during cache operations
  - [ ] Optimize cache data structure for minimal memory footprint

- [ ] **Task 6.2: Implement Cache Compression**
  - [ ] Evaluate compression algorithms (zlib, gzip, lz4, zstd)
  - [ ] Implement configurable compression (on/off, algorithm choice)
  - [ ] Measure compression ratio vs speed trade-offs
  - [ ] Implement selective compression (compress large caches only)
  - [ ] Test memory usage with compressed vs uncompressed caches
  - [ ] Document compression recommendations

- [ ] **Task 6.3: Memory Usage Testing and Validation**
  - [ ] Measure memory usage with typical knowledge bases
  - [ ] Measure memory usage with large knowledge bases (stress test)
  - [ ] Verify memory usage stays within NFR8 limit (≤6GB)
  - [ ] Identify and fix memory leaks
  - [ ] Profile and optimize memory hotspots
  - [ ] Document memory usage characteristics

### Phase 7: Testing and Documentation (AC: 10, 11)

- [ ] **Task 7.1: Create Unit Tests for Cache Operations**
  - [ ] Create test/test_cache_manager.rb
  - [ ] Test cache precomputation with sample contexts
  - [ ] Test cache serialization and deserialization
  - [ ] Test cache save and load operations
  - [ ] Test cache versioning and compatibility checks
  - [ ] Test cache invalidation and cleanup
  - [ ] Test incremental cache updates

- [ ] **Task 7.2: Create Integration Tests**
  - [ ] Test end-to-end cache workflow (precompute -> save -> load -> use)
  - [ ] Test cache operations with different LLM providers
  - [ ] Test cache operations with different knowledge base sizes
  - [ ] Test concurrent cache access (if applicable)
  - [ ] Test cache operations in offline mode
  - [ ] Test cache migration across versions

- [ ] **Task 7.3: Create Error Handling Tests**
  - [ ] Test cache corruption scenarios
  - [ ] Test missing cache file scenarios
  - [ ] Test version mismatch scenarios
  - [ ] Test disk I/O error scenarios
  - [ ] Test memory limit scenarios
  - [ ] Verify all errors logged and handled gracefully

- [ ] **Task 7.4: Measure Test Coverage**
  - [ ] Run tests with coverage tool (SimpleCov)
  - [ ] Verify >80% coverage for cache_manager.rb
  - [ ] Identify untested code paths
  - [ ] Add tests for uncovered code
  - [ ] Document any intentionally untested code

- [ ] **Task 7.5: Run All Tests in Nix Environment**
  - [ ] Execute full test suite: `ruby test/test_cache_manager.rb`
  - [ ] Verify all tests pass
  - [ ] Fix any failures
  - [ ] Verify tests complete in reasonable time
  - [ ] Document any environment-specific requirements

- [ ] **Task 7.6: Integration Verification** (IV1-IV5)
  - [ ] **IV1**: Verify cache files in correct directory with proper permissions
  - [ ] **IV2**: Verify cache loading doesn't block normal operations
  - [ ] **IV3**: Run cache operations offline, verify no external dependencies
  - [ ] **IV4**: Measure memory usage, verify ≤6GB constraint met
  - [ ] **IV5**: Measure cache precomputation time, verify ≤5 minutes

---

## Dev Notes

### Cache Manager Architecture

**CacheManager** (`cag/cache_manager.rb`):
- **Purpose**: Manage KV cache lifecycle for CAG system
- **Key Responsibilities**:
  - Precompute KV caches from assembled contexts
  - Persist caches to disk with versioning and integrity
  - Load caches efficiently for fast query response
  - Handle cache invalidation and updates
  - Optimize memory usage throughout cache operations
- **Integration Points**:
  - Context Manager: Receives assembled contexts for precomputation
  - LLM Clients: Interfaces with LLM providers to extract KV caches
  - Inference Engine: Provides cached context for query generation
  - Knowledge Base Loader: Receives updates for cache invalidation

[Source: docs/stories/epic-2-cag-system-implementation.md#42-76]

### KV Cache Concepts (Technical Background)

**What is KV Cache?**
- **Definition**: Key-Value cache stores pre-computed attention parameters (keys and values) from transformer model layers
- **Purpose**: Eliminates need to recompute attention for already-processed tokens during inference
- **Benefit**: Dramatically reduces inference time for long contexts (50-80% faster)
- **Structure**: Typically multi-dimensional tensors per layer: `[num_layers, 2, batch_size, num_heads, seq_len, head_dim]`

**KV Cache in CAG Context**:
- Precompute KV cache for entire knowledge base context (all documents)
- Store computed cache persistently (disk storage)
- Load cached context at query time (no real-time document retrieval)
- Append query to cached context for generation (minimal additional computation)

**Model-Specific Considerations**:
- **Ollama**: KV cache extraction may require API extensions or model-specific handling
- **OpenAI**: API doesn't expose KV cache directly; may need alternative approach (context pre-loading)
- **VLLM**: Supports KV cache management through API
- **SGLang**: Designed for fast LLM serving with KV cache optimization

**Memory Considerations**:
- KV cache size proportional to: `num_layers × hidden_size × context_length`
- Example: 32 layers × 4096 hidden × 32k context ≈ 4-8GB (FP16)
- Compression and quantization can reduce memory usage
- Memory-mapped storage enables larger caches than available RAM

[Source: Modern CAG research, transformer architecture documentation]

### Binary Storage Formats

**Format Options**:

1. **MessagePack** (Recommended)
   - Efficient binary serialization
   - Language-agnostic (Ruby gem available)
   - Compact representation
   - Fast serialization/deserialization
   - Example: `msgpack` gem

2. **CBOR (Concise Binary Object Representation)**
   - Similar to MessagePack
   - Internet standard (RFC 7049)
   - Good Ruby support (`cbor` gem)
   - Slightly more features than MessagePack

3. **Custom Binary Format**
   - Maximum control over layout
   - Optimized for specific use case
   - Requires custom serialization code
   - Consider if standard formats insufficient

**Recommended Approach**: Start with MessagePack for simplicity and performance

**Storage Structure**:
```ruby
cache_file = {
  version: "1.0.0",           # Cache format version
  timestamp: Time.now.to_i,    # Cache creation time
  model_info: {                # LLM model information
    provider: "ollama",
    model_name: "llama3.1:70b",
    context_window: 131072
  },
  knowledge_base_hash: "abc123...",  # Fingerprint of knowledge base
  document_count: 150,         # Number of documents in cache
  cache_data: {                # Actual KV cache data
    # Model-specific KV cache structure
    # Could be tensors, arrays, or provider-specific format
  },
  metadata: {                  # Additional metadata
    compression: "zlib",
    checksum: "sha256:..."
  }
}
```

[Source: MessagePack, CBOR documentation, CAG implementation best practices]

### Cache Versioning Strategy

**Semantic Versioning**:
- **Major**: Breaking changes to cache format (incompatible)
- **Minor**: Backward-compatible additions (can load old caches)
- **Patch**: Bug fixes, no format changes (fully compatible)

**Version Compatibility Matrix**:
```ruby
CACHE_VERSION = "1.0.0"

def compatible?(cache_version)
  cache_major, cache_minor, _ = cache_version.split('.').map(&:to_i)
  current_major, current_minor, _ = CACHE_VERSION.split('.').map(&:to_i)

  # Same major version required
  return false if cache_major != current_major

  # Current version must be >= cache version for minor
  current_minor >= cache_minor
end
```

**Version Migration**:
- Automatic migration for minor version bumps (add default values)
- Reject caches from different major versions (regenerate required)
- Log version migrations for debugging
- Consider maintaining migration utilities for important version jumps

[Source: Semantic versioning best practices, cache management patterns]

### Cache Invalidation Strategies

**Strategy 1: Time-Based Invalidation**
- Set cache expiration time (e.g., 7 days, 30 days)
- Simple to implement and understand
- Good for knowledge bases that change infrequently
- May invalidate cache unnecessarily if no changes

**Strategy 2: Content-Based Invalidation**
- Calculate hash/fingerprint of knowledge base content
- Compare with cached fingerprint on load
- Regenerate cache if mismatch detected
- More accurate but requires hashing all content

**Strategy 3: Manual Invalidation**
- User/admin triggers cache regeneration
- Useful for development and testing
- Gives explicit control over cache lifecycle
- Requires user awareness and action

**Strategy 4: Hybrid Approach (Recommended)**
- Combine time-based and content-based strategies
- Check content hash on cache load
- Also enforce maximum cache age
- Provides safety net and accuracy

**Implementation Example**:
```ruby
def cache_valid?(cache_metadata)
  # Check time-based expiration
  cache_age = Time.now.to_i - cache_metadata[:timestamp]
  return false if cache_age > MAX_CACHE_AGE

  # Check content-based invalidation
  current_kb_hash = calculate_knowledge_base_hash
  return false if current_kb_hash != cache_metadata[:knowledge_base_hash]

  true
end
```

[Source: Caching best practices, Epic 2 risk mitigation strategies]

### Incremental Cache Updates

**Full Recomputation vs Incremental Update**:

**Full Recomputation**:
- Regenerate entire KV cache from scratch
- Required when: significant knowledge base changes, model changes
- Time: Up to 5 minutes (NFR9)
- Simple and reliable

**Incremental Update**:
- Append new documents to existing cached context
- Recompute KV cache only for new content
- Time: Proportional to new content (faster for small updates)
- More complex, model-dependent

**Incremental Update Challenges**:
- KV cache structure may not support simple appending
- Model-specific limitations (some models don't support partial cache)
- Context window limitations (may exceed max context)
- Cache coherence (ensure new cache compatible with old)

**Recommended Approach**:
- Start with full recomputation (simpler, reliable)
- Add incremental updates as optimization (if model supports)
- Document when incremental updates possible vs full recompute needed
- Provide user feedback on cache update progress

**Incremental Update Pseudocode**:
```ruby
def update_cache(cache_id, new_documents)
  # Load existing cache
  cache = load_cache(cache_id)

  # Check if incremental update feasible
  if incremental_update_possible?(cache, new_documents)
    # Append new documents to context
    updated_context = append_to_context(cache.context, new_documents)

    # Recompute KV cache incrementally
    updated_cache = precompute_cache_incremental(cache.kv_cache, new_documents)
  else
    # Fall back to full recomputation
    updated_context = rebuild_full_context(cache, new_documents)
    updated_cache = precompute_cache(updated_context)
  end

  # Save updated cache
  save_cache(updated_cache, cache_id)
end
```

[Source: KV cache optimization techniques, incremental computation patterns]

### Memory Efficiency Techniques

**Technique 1: Streaming I/O**
- Don't load entire cache into memory at once
- Use streaming deserialization for large cache files
- Process cache data in chunks
- Ruby example: `File.foreach` or custom buffered reading

**Technique 2: Memory-Mapped Files**
- Use `mmap` to map cache file to virtual memory
- OS handles paging to/from disk automatically
- Reduces peak memory usage for large caches
- Ruby gem: `mmap` or native IO capabilities

**Technique 3: Lazy Loading**
- Load cache metadata first (small)
- Defer loading full cache data until needed
- Useful when inspecting multiple caches
- Reduces memory for unused caches

**Technique 4: Compression**
- Compress cache data before storage
- Reduces disk usage and I/O time
- CPU vs memory trade-off
- Options: zlib (good balance), lz4 (fast), zstd (best ratio)

**Technique 5: Data Structure Optimization**
- Use arrays instead of hashes where possible (lower overhead)
- Pack numeric data efficiently (binary instead of text)
- Remove redundant metadata
- Use appropriate numeric types (Float vs Integer)

**Memory Profiling**:
```ruby
require 'memory_profiler'

report = MemoryProfiler.report do
  cache = load_cache("test_cache")
end

report.pretty_print
```

[Source: Ruby memory optimization techniques, large file handling best practices]

### LLM Provider Integration

**Provider-Specific Cache Handling**:

**Ollama**:
- API may not directly expose KV cache extraction
- Possible approaches:
  - Use model-specific API extensions (if available)
  - Pre-load context and use prompt caching
  - Coordinate with Ollama server for cache persistence
- Need to research Ollama capabilities for KV cache access

**OpenAI**:
- API doesn't expose internal KV cache
- Alternative approach: Use prompt caching feature
- Pre-load context as system message or prefix
- May not provide same performance as native KV cache
- Consider if CAG approach viable with OpenAI

**VLLM**:
- Better support for KV cache management
- API may expose cache control parameters
- Designed for efficient LLM serving
- Good candidate for CAG implementation

**SGLang**:
- Designed for fast LLM serving with KV cache optimization
- Likely best provider support for CAG
- May have native KV cache save/load APIs
- Research SGLang capabilities for cache management

**Recommended Strategy**:
- Start with provider that has best KV cache support (VLLM or SGLang)
- Abstract cache interface to support multiple providers
- Document provider-specific limitations
- Fallback to context pre-loading for providers without KV cache access

[Source: LLM provider documentation, CAG implementation research]

### Error Handling Patterns

**Pattern 1: Graceful Degradation**
```ruby
def load_cache(cache_id)
  cache = deserialize_cache(cache_id)
rescue CacheCorruptionError => e
  Print.err "Cache corrupted, regenerating: #{e.message}"
  regenerate_cache(cache_id)
rescue CacheNotFoundError => e
  Print.warn "Cache not found, generating: #{e.message}"
  generate_cache(cache_id)
end
```

**Pattern 2: Detailed Error Logging**
```ruby
def save_cache(cache_data, path)
  serialize_and_write(cache_data, path)
rescue Errno::ENOSPC => e
  Print.err "Disk full while saving cache: #{path}"
  Print.err "Error details: #{e.message}"
  raise CacheSaveError, "Insufficient disk space"
rescue Errno::EACCES => e
  Print.err "Permission denied while saving cache: #{path}"
  Print.err "Error details: #{e.message}"
  raise CacheSaveError, "Insufficient permissions"
end
```

**Pattern 3: Recovery and Retry**
```ruby
def precompute_cache_with_retry(context, max_retries: 3)
  retries = 0
  begin
    precompute_cache(context)
  rescue TransientError => e
    retries += 1
    if retries <= max_retries
      Print.warn "Cache precomputation failed, retrying (#{retries}/#{max_retries})"
      sleep(2 ** retries)  # Exponential backoff
      retry
    else
      raise CachePrecomputeError, "Failed after #{max_retries} retries"
    end
  end
end
```

[Source: Ruby error handling best practices, resilient system design]

### Performance Targets (from NFRs)

**Cache-Related NFRs**:

- **NFR8**: Memory usage for CAG system ≤ 6GB for typical knowledge base sizes
- **NFR9**: Cache precomputation time ≤ 5 minutes for initial knowledge base population
- **NFR10**: CAG query response times ≤ 2 seconds (excluding LLM inference)
- **Cache loading target**: ≤ 30 seconds from stored cache (from Epic description)

**Measurement Strategy**:
- Benchmark with typical knowledge base (MITRE ATT&CK + man pages + labs)
- Measure peak memory during precomputation and loading
- Measure wall-clock time for cache operations
- Test with various knowledge base sizes (small, medium, large)
- Document performance characteristics in test results

**Optimization Priorities**:
1. Meet memory constraints (NFR8) - CRITICAL
2. Meet precomputation time (NFR9) - CRITICAL
3. Meet cache loading time (<30s) - HIGH
4. Optimize query response (NFR10) - Depends on inference engine integration

[Source: docs/prd.md#2.2 Non-Functional Requirements, Epic 2 performance constraints]

### Cache Directory Structure

**Recommended Layout**:
```
opt_hackerbot/
├── caches/                          # Cache storage directory
│   ├── default/                     # Default knowledge base cache
│   │   ├── cache_v1.0.0_ollama_llama3.1.bin
│   │   └── metadata.json
│   ├── mitre_only/                  # MITRE ATT&CK only cache
│   │   ├── cache_v1.0.0_ollama_llama3.1.bin
│   │   └── metadata.json
│   └── custom/                      # User-defined caches
│       └── ...
└── cag/
    └── cache_manager.rb
```

**File Naming Convention**:
- Format: `cache_v{VERSION}_{PROVIDER}_{MODEL}.bin`
- Example: `cache_v1.0.0_ollama_llama3.1-70b.bin`
- Metadata: `metadata.json` (human-readable cache info)

**Permissions**:
- Cache directory: `drwxr-xr-x` (755)
- Cache files: `-rw-r--r--` (644)
- Ensure bot process has write access

[Source: Unix file system best practices, Ruby file handling patterns]

### Testing Strategy

**Unit Tests** (test/test_cache_manager.rb):
- Test each public method independently
- Mock external dependencies (LLM clients, file I/O)
- Test edge cases and error conditions
- Fast execution (no actual cache precomputation)

**Integration Tests** (within test_cache_manager.rb or separate):
- Test full cache workflow end-to-end
- Use small test knowledge base for speed
- Test with actual LLM client (if available in test env)
- Test cache persistence across restarts

**Performance Tests** (test/test_cag_performance.rb - Story 2.8):
- Measure cache operation performance
- Verify NFR compliance
- Benchmark with various knowledge base sizes
- Test memory usage and limits

**Test Fixtures**:
- Small test knowledge base (5-10 documents)
- Sample cache files (various versions, states)
- Corrupted cache file for error testing
- Mock LLM responses for cache precomputation

**Coverage Target**: >80% for cache_manager.rb

[Source: Story template, Epic 2 testing requirements]

### Code Organization

**CacheManager Class Structure**:
```ruby
# cag/cache_manager.rb

class CacheManager
  CACHE_VERSION = "1.0.0"
  DEFAULT_CACHE_DIR = "caches"

  # Initialization
  def initialize(config = {})
    # Setup cache directory, compression, etc.
  end

  # Cache Precomputation
  def precompute_cache(context, model_info)
    # Interface with LLM to compute KV cache
  end

  # Cache Persistence
  def save_cache(cache_data, cache_id, metadata = {})
    # Serialize and save cache to disk
  end

  def load_cache(cache_id)
    # Load and deserialize cache from disk
  end

  # Cache Management
  def invalidate_cache(cache_id)
    # Mark cache as invalid or delete
  end

  def update_cache(cache_id, new_documents)
    # Incremental cache update
  end

  def list_caches
    # Enumerate available caches
  end

  def get_cache_metadata(cache_id)
    # Get cache information without loading full cache
  end

  # Utilities
  def cache_valid?(cache_id)
    # Check if cache is valid (not expired, compatible, etc.)
  end

  def cleanup_caches
    # Remove old/invalid caches
  end

  private

  # Serialization
  def serialize_cache(cache_data, metadata)
    # Convert cache to binary format
  end

  def deserialize_cache(binary_data)
    # Parse binary format to cache object
  end

  # Version Management
  def check_version_compatibility(cache_version)
    # Verify cache version compatible with current code
  end

  # Knowledge Base Fingerprinting
  def calculate_knowledge_base_hash
    # Compute hash of current knowledge base
  end

  # Error Handling
  def handle_cache_error(error, cache_id)
    # Centralized error handling and logging
  end
end
```

[Source: Ruby class design patterns, Epic 2 architecture]

### Dependencies and Imports

**Required Gems**:
- `msgpack` - Binary serialization (or `cbor` alternative)
- `zlib` - Compression (standard library)
- `digest` - Hashing for checksums (standard library)
- `fileutils` - File operations (standard library)
- `memory_profiler` - Memory profiling for optimization (dev only)

**Internal Dependencies**:
- `cag/context_manager.rb` - Provides assembled context for precomputation
- `llm_client_factory.rb` - Access to LLM clients for KV cache extraction
- `print.rb` - Logging utility
- Knowledge base interfaces - For change detection and invalidation

**Installation**:
```bash
gem install msgpack  # If using MessagePack
# or
gem install cbor     # If using CBOR
```

[Source: Ruby gem ecosystem, Epic 2 architecture]

---

## Testing

### Testing Strategy for This Story

**Unit Testing Focus**:
1. **Cache Serialization**: Test binary format conversion correctness
2. **Cache Persistence**: Test save/load operations with various cache sizes
3. **Cache Versioning**: Test version compatibility checks and migrations
4. **Cache Invalidation**: Test invalidation triggers and strategies
5. **Error Handling**: Test graceful handling of corruption, missing files, etc.
6. **Memory Efficiency**: Test memory usage stays within constraints

**Integration Testing Focus**:
1. **End-to-End Workflow**: Precompute → Save → Load → Use cache
2. **LLM Provider Integration**: Test with available providers (Ollama, etc.)
3. **Knowledge Base Integration**: Test with actual knowledge sources
4. **Incremental Updates**: Test adding documents to existing cache

**Performance Testing Focus**:
1. **Precomputation Time**: Verify ≤5 minutes for typical knowledge base
2. **Cache Loading Time**: Verify ≤30 seconds for cache loading
3. **Memory Usage**: Verify ≤6GB during all operations
4. **Disk Usage**: Measure cache file sizes with various configurations

### Test Execution Commands

```bash
# Run cache manager unit tests
ruby test/test_cache_manager.rb

# Run with verbose output
ruby test/test_cache_manager.rb --verbose

# Run with coverage measurement
ruby test/test_cache_manager.rb --coverage

# Run in Nix environment
nix develop
ruby test/test_cache_manager.rb

# Memory profiling
ruby -r memory_profiler test/test_cache_manager.rb

# Integration verification
ruby test/test_cache_manager.rb --integration
```

### Success Criteria

✅ All unit tests pass with >80% coverage
✅ Cache precomputation completes within NFR9 limit (≤5 minutes)
✅ Cache loading completes within target (≤30 seconds)
✅ Memory usage within NFR8 limit (≤6GB)
✅ Cache persistence works correctly (save/load roundtrip)
✅ Error handling graceful for all failure scenarios
✅ Integration verification complete (IV1-IV5)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | v1.0 | Initial story creation for Cache Manager implementation | SM Agent |

---

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

---

## QA Results

_This section will be populated by QA Agent after story completion._

---

**Story prepared by**: Scrum Master Agent
**Ready for**: Developer Agent implementation (after Story 2.3 completion)
**Next Story**: 2.5 - Implement CAG Inference Engine (depends on this story completion)
